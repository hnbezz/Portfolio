{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error, accuracy_score\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "import seaborn as sns\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Seatle's data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_l_data = pd.read_csv('Seatle_listings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the listings data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing url features\n",
    "col_url = []\n",
    "for col in S_l_data.columns.tolist():\n",
    "    if '_url' in col:\n",
    "        col_url.append(col)\n",
    "\n",
    "S_l_dropped2 = S_l_data.drop(col_url, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I am removing some features that obviously lead to nothing, such as #id and 'name'.**\n",
    "\n",
    "**I have to explain some of the removals, such as 'street', 'zipcode', 'host_listings_count', 'latitude', 'neighbourhood_cleansed', neighbourhood_group_cleansed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_l_dropped3 = S_l_dropped2.drop(['#id','name','host_id','host_name','host_id','smart_location','state', \n",
    "                                  'host_listings_count','street', 'host_location', 'neighbourhood',\n",
    "                                  'host_neighbourhood', 'neighbourhood_group_cleansed','neighbourhood_cleansed','city'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I also need to transform the dates that are in string format to date format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = re.compile('.{4}-.{2}-.{2}')\n",
    "\n",
    "for col in S_l_dropped3.columns.tolist():\n",
    "    if S_l_dropped3[col].dtypes == 'O':\n",
    "        if r.match(S_l_dropped3[col].any()):\n",
    "            S_l_dropped3[col] = pd.to_datetime(S_l_dropped3[col], errors='ignore')\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I will also drop features that I cannot really evaluate considering the knowledge I have. However I will create a variable to check if the feature is offered or not. For example, there are some listings that do not inform the \"Neighborhood overview\", So i will drop the overview and create a boolean variable called \"neighborhood_overview_given\" to indicate if it was informed. This I do manually, because I can't find a way to do it systematically for the different features.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing features which do no vary.\n",
    "col_to_drop = []\n",
    "for col in S_l_dropped3.columns.tolist():\n",
    "    if len(S_l_dropped3[col].unique())==1:\n",
    "        col_to_drop.append(col)\n",
    "\n",
    "S_l_dropped4 = S_l_dropped3.drop(col_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I'll remove the outliers here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_vars1 =  S_l_dropped4.select_dtypes(include=['float', 'int']).columns\n",
    "for col in num_vars1:\n",
    "    q = S_l_dropped4[col].quantile(0.99)\n",
    "    S_l_dropped5 = S_l_dropped4[S_l_dropped4[col]<q]\n",
    "S_l_amenver = S_l_dropped5.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_l_amenver['host_since'] = (S_l_amenver['host_since']-S_l_amenver['host_since'].min()).dt.days\n",
    "S_l_amenver['first_review'] = (S_l_amenver['first_review']-S_l_amenver['first_review'].min()).dt.days\n",
    "S_l_amenver['last_review'] = (S_l_amenver['last_review']-S_l_amenver['last_review'].min()).dt.days\n",
    "\n",
    "for i in range(S_l_amenver.shape[0]):\n",
    "    if 'weeks ago' in S_l_amenver['calendar_updated'][i]:\n",
    "        S_l_amenver.loc[i,'calendar_updated'] = float(S_l_amenver['calendar_updated'][i].replace(' weeks ago', '')) * 7\n",
    "    elif 'a week ago' in S_l_amenver['calendar_updated'][i]:\n",
    "        S_l_amenver.loc[i,'calendar_updated'] = float(S_l_amenver['calendar_updated'][i].replace('a week ago', '1')) * 7\n",
    "    elif '1 week ago' in S_l_amenver['calendar_updated'][i]:\n",
    "        S_l_amenver.loc[i,'calendar_updated'] = float(S_l_amenver['calendar_updated'][i].replace('1 week ago', '1')) * 7\n",
    "    elif 'yesterday' in S_l_amenver['calendar_updated'][i]:\n",
    "        S_l_amenver.loc[i,'calendar_updated'] = float(S_l_amenver['calendar_updated'][i].replace('yesterday', '1')) * 1\n",
    "    elif 'days ago' in S_l_amenver['calendar_updated'][i]:\n",
    "        S_l_amenver.loc[i,'calendar_updated'] = float(S_l_amenver['calendar_updated'][i].replace(' days ago', '1')) * 1\n",
    "    elif 'today' in S_l_amenver['calendar_updated'][i]:\n",
    "        S_l_amenver.loc[i,'calendar_updated'] = float(S_l_amenver['calendar_updated'][i].replace('today', '1')) * 1\n",
    "    elif 'never' in S_l_amenver['calendar_updated'][i]:\n",
    "        S_l_amenver.loc[i,'calendar_updated'] = float(S_l_amenver['calendar_updated'][i].replace('never', '1')) * 2000\n",
    "    elif 'months ago' in S_l_amenver['calendar_updated'][i]:\n",
    "        S_l_amenver.loc[i,'calendar_updated'] = float(S_l_amenver['calendar_updated'][i].replace(' months ago', '')) * 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are some numbers that have '$' or '\\%' that are seen as strings, so I fix this.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_vars = S_l_amenver.select_dtypes(include=['object', 'O']).copy().columns\n",
    "for col in col_vars:\n",
    "    if '$' in S_l_amenver[col].any():\n",
    "        S_l_amenver[col] = S_l_amenver[col].replace('[\\$,]', '', regex=True).astype(float)\n",
    "\n",
    "col_vars2 = S_l_amenver.select_dtypes(include=['object', 'O']).copy().columns\n",
    "for col in col_vars2:\n",
    "    if '%' in S_l_amenver[col].any():\n",
    "        S_l_amenver[col] = S_l_amenver[col].replace('[,\\%]','',regex=True).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I'll remove any categorical variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vars0 = S_l_amenver.select_dtypes(include=['object', 'O']).copy().columns\n",
    "S_l_amenver = S_l_amenver.drop(cat_vars0,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I define some functions here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_fill(df,y_data,drop_data):\n",
    "    \"\"\"\n",
    "    df: the pandas dataframe from where the data comes from\n",
    "    y_data: the string of the dependent variable name\n",
    "    drop: A list of strings with the features you would like to drop\n",
    "    \n",
    "    Outputs:\n",
    "    X: pandas data without NaN, the independent variables\n",
    "    y: pandas data without Nan, the dependent variable\n",
    "    num_vars: list of strings with numerical variables.\n",
    "    \n",
    "    This function fills missing values with means for numerical, and mode for categorical data.\n",
    "    \"\"\"\n",
    "    drop_data.append(y_data)\n",
    "    X = df.drop(drop_data, axis=1)\n",
    "    y = df[y_data]\n",
    "    y.fillna((y.mean()), inplace=True)\n",
    "    \n",
    "    num_vars = X.select_dtypes(include=['float', 'int']).columns\n",
    "    for col in num_vars:\n",
    "        X[col].fillna((X[col].mean()), inplace=True)\n",
    "    \n",
    "    cat_vars = X.select_dtypes(include=['object', 'O']).copy().columns\n",
    "    for var in cat_vars:\n",
    "        X[var].fillna((X[var].mode()[0]), inplace=True)\n",
    "    return X, y, num_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dummies(df):\n",
    "    cat_vars = df.select_dtypes(include=['object', 'O']).copy().columns\n",
    "    for var in cat_vars:\n",
    "        # for each cat add dummy var, drop original column\n",
    "        df = pd.concat([df.drop(var, axis=1), pd.get_dummies(df[var], prefix=var, prefix_sep='_', drop_first=True)], axis=1)\n",
    "    df_2= df.copy()\n",
    "    return df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vif_dropping(X_data,VIF_lim):\n",
    "    \"\"\"\n",
    "    X_data: pandas data, independent variables without num values.\n",
    "    VIF_lim: the maximum VIF value allowed to exist\n",
    "    \n",
    "    X_out: pandas dataframe without features VIF values above VIF_lim.\n",
    "    \"\"\"\n",
    "    num_vars2 = X_data.select_dtypes(include=['float', 'int']).columns\n",
    "    variables = X_data[num_vars2]\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"VIF\"] = [variance_inflation_factor(variables.values, i) for i in range(variables.shape[1])]\n",
    "    vif_val=50\n",
    "    \n",
    "    while len(vif[vif['VIF']>VIF_lim]) != 0:\n",
    "        num_vars2 = X_data.select_dtypes(include=['float', 'int']).columns\n",
    "        vif_val=vif_val-5\n",
    "        variables = X_data[num_vars2]\n",
    "        vif = pd.DataFrame()\n",
    "        vif[\"VIF\"] = [variance_inflation_factor(variables.values, i) for i in range(variables.shape[1])]\n",
    "        vif[\"Features\"] = variables.columns\n",
    "        VIF_clean=vif[vif[\"VIF\"]>vif_val].Features\n",
    "        X_data = X_data.drop(VIF_clean,axis=1)\n",
    "        \n",
    "    X_out = X_data.copy()\n",
    "    return X_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LR_model(X_data,y_data,test_sizee, rdm_state):\n",
    "    \"\"\"\n",
    "    X_data: pandas dataframe with the independent variables\n",
    "    y_data: pandas dataframe with the dependent variable\n",
    "    test_sizee: size of the test sample\n",
    "    rdm_state: random state for randomizer.\n",
    "    \n",
    "    y_test: the test targets, as a numpy array.\n",
    "    y_test_preds: the predicted targets, as a numpy array.\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = X_data.copy()\n",
    "    scaler.fit(X_scaled)\n",
    "    scaler.transform(X_scaled)\n",
    "    \n",
    "    X_columns = X_data.columns.tolist()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(scaler.transform(X_scaled), y_data, test_size = test_sizee, random_state=rdm_state) \n",
    "    lm_model = LinearRegression() \n",
    "    lm_model.fit(X_train, y_train) \n",
    "\n",
    "    y_test_preds = lm_model.predict(X_test) \n",
    "    \n",
    "    return X_columns, X_train, X_test, y_train, y_test, y_test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing features which do no vary.\n",
    "col_to_drop = []\n",
    "for col in S_l_amenver.columns.tolist():\n",
    "    if len(S_l_amenver[col].unique())==1:\n",
    "        col_to_drop.append(col)\n",
    "\n",
    "S_l_amenver = S_l_amenver.drop(col_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying the Linear Model with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y,num_vars = drop_fill(S_l_amenver,'price',['weekly_price','monthly_price'])\n",
    "#X = make_dummies(X)\n",
    "# X = vif_dropping(X,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The r-squared score for the model using all variables was 0.6409450363698318 on 632 values.'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_columns, X_train, X_test, y_train, y_test, y_test_preds = LR_model(X,y,0.2,26) \n",
    "\"The r-squared score for the model using all variables was {} on {} values.\".format(r2_score(y_test, y_test_preds), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3159, 37)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_l_amenver.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6257384634552523"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVR(kernel='linear',C=1,epsilon=0.1)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[110. 100. 175.  85. 127. 125. 129.  51. 245.  50.  69.  99. 149.  83.\n  31. 105.  70.  47. 110. 142.  90.  90.  70.  75. 100.  62. 122.  55.\n  85. 150.  65.  75. 120.  80. 150. 325.  69.  60. 159.  55. 155.  65.\n 195. 115.  58. 110.  75. 100. 115. 135. 119. 179. 150.  98. 498. 149.\n 110. 125. 115. 175. 100. 119. 135. 175.  75.  53. 150. 200. 105.  75.\n  60. 175. 150. 120. 126. 120. 129.  99. 350.  45.  90. 465.  69.  99.\n 125. 150.  78. 100. 149.  90. 197. 150.  58. 100. 104.  99. 399.  90.\n 149. 110. 405. 175.  99. 199. 109.  79. 155. 700.  55.  95.  97.  65.\n 115.  95. 175. 110.  70. 160. 126.  80. 320.  80. 450. 150.  68.  99.\n 109. 100.  89. 105.  88. 250.  65.  99.  65.  40.  60.  67. 135.  75.\n 175.  65. 130.  53.  65.  93.  75. 115.  85.  55. 139. 135. 375. 115.\n  40.  90. 224. 130. 140.  65. 149. 195.  84.  99. 225.  70. 285. 200.\n 250.  69.  85. 300. 125.  40. 129.  65. 115. 225. 129. 155. 100.  40.\n  95. 110. 100. 150. 170.  40. 225.  92.  75.  40. 250. 158.  99.  95.\n 115. 175.  78.  69.  45. 250.  95. 100. 159. 120.  81.  45.  39. 114.\n  40. 200. 100. 130. 110. 245. 240.  99.  65. 174. 175. 135.  90. 175.\n  65. 239.  63. 190.  52.  85. 190. 117.  65. 125.  75. 159. 125.  55.\n 119.  68. 115. 100.  70. 175. 210.  29. 129. 160. 125. 130.  89.  50.\n  95. 350.  75.  95.  35. 120. 350.  55.  91. 200. 275.  98.  90. 125.\n  90. 229. 150.  65. 170. 110.  41. 100. 275. 145. 115.  80. 115. 105.\n 195.  99.  64. 100. 239.  90. 108.  50.  68.  99. 175.  29. 126. 125.\n  80. 150.  75. 195.  68. 120.  58. 168.  95. 300. 110.  37. 149.  80.\n 100.  80.  42. 125.  70. 349.  43. 183.  39. 100. 119.  80.  88.  95.\n 140. 375. 149. 137.  47. 145.  80. 139.  70. 145. 130. 109.  39. 157.\n 110.  80.  85.  60. 130. 200.  49. 156. 150.  40.  50.  85.  59. 165.\n 150. 170.  95. 115.  88. 300.  49.  50. 166. 125.  65. 139.  49.  85.\n 110.  28. 125. 109. 150.  65. 399. 120. 100. 140. 120. 129.  75.  80.\n  55. 140. 139.  85. 280. 115.  95. 225. 220.  90. 200.  90.  65.  75.\n  75.  85.  90. 115. 257.  98. 165. 175.  90.  60. 165. 175. 175.  89.\n  95. 215.  68.  75. 175. 150. 115. 170. 139.  95. 150.  45. 178.  85.\n 119. 120.  69. 175. 115.  68.  75.  75. 105.  75.  45. 125. 115.  75.\n 125.  52.  35. 250. 119. 100. 199.  85.  60. 110. 129.  87.  69. 250.\n  65. 150.  99.  75. 130. 135.  66.  50. 162.  50.  85. 250.  50. 200.\n  57. 145. 150.  80. 195.  70. 139. 240.  59.  95.  45. 209. 100. 108.\n  56.  80. 225. 195. 100.  49. 219.  95.  59. 180. 445.  90.  45.  30.\n  85. 120. 137. 141.  50. 250. 335. 159. 109. 350. 115. 110.  89. 118.\n 149. 129. 151. 360. 170.  96.  90. 175. 200. 149. 125.  50. 129.  90.\n 338.  97.  65. 330.  55. 100.  49.  41. 150. 550. 175. 175. 169. 175.\n  65. 150.  86.  53. 500. 129. 325.  55.  45. 300. 155. 245. 439. 115.\n 119.  40. 150. 200.  95. 215.  75.  75. 108. 136.  59.  75.  65.  90.\n 120.  40. 175. 129.  55. 117.  95. 110.  65. 174. 150. 200.  99.  99.\n  55.  90. 117. 105. 100. 200.  90.  80.  68. 144.  85.  85. 111.  95.\n 150.  38. 110. 350. 150.  70. 105.  71.  49. 100.  37. 215.  40. 120.\n 325. 139.  80.  70. 139. 119.  38. 132. 110.  85. 135. 129.  80. 300.\n 120. 250.  69.  62. 120.  66.  69.  35. 119.  90.  39.  65. 100. 100.\n  40.  65.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-81-ee1b2c262036>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mregr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0my_ada_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mregr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_ada_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    420\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_regression\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_check_reg_targets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 422\u001b[1;33m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    423\u001b[0m         \u001b[1;31m# XXX: Remove the check in 0.23\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m         \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_reg_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1129\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_median_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstaged_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\u001b[0m in \u001b[0;36m_get_median_predict\u001b[1;34m(self, X, limit)\u001b[0m\n\u001b[0;32m   1094\u001b[0m         \u001b[1;31m# Evaluate predictions of all estimators\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1095\u001b[0m         predictions = np.array([\n\u001b[1;32m-> 1096\u001b[1;33m             est.predict(X) for est in self.estimators_[:limit]]).T\n\u001b[0m\u001b[0;32m   1097\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m         \u001b[1;31m# Sort the predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1094\u001b[0m         \u001b[1;31m# Evaluate predictions of all estimators\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1095\u001b[0m         predictions = np.array([\n\u001b[1;32m-> 1096\u001b[1;33m             est.predict(X) for est in self.estimators_[:limit]]).T\n\u001b[0m\u001b[0;32m   1097\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m         \u001b[1;31m# Sort the predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    417\u001b[0m         \"\"\"\n\u001b[0;32m    418\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 419\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    420\u001b[0m         \u001b[0mproba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    378\u001b[0m         \u001b[1;34m\"\"\"Validate X whenever one tries to predict, apply, predict_proba\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 380\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csr\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    381\u001b[0m             if issparse(X) and (X.indices.dtype != np.intc or\n\u001b[0;32m    382\u001b[0m                                 X.indptr.dtype != np.intc):\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    554\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m                     \u001b[1;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 556\u001b[1;33m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[0;32m    557\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    558\u001b[0m         \u001b[1;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[110. 100. 175.  85. 127. 125. 129.  51. 245.  50.  69.  99. 149.  83.\n  31. 105.  70.  47. 110. 142.  90.  90.  70.  75. 100.  62. 122.  55.\n  85. 150.  65.  75. 120.  80. 150. 325.  69.  60. 159.  55. 155.  65.\n 195. 115.  58. 110.  75. 100. 115. 135. 119. 179. 150.  98. 498. 149.\n 110. 125. 115. 175. 100. 119. 135. 175.  75.  53. 150. 200. 105.  75.\n  60. 175. 150. 120. 126. 120. 129.  99. 350.  45.  90. 465.  69.  99.\n 125. 150.  78. 100. 149.  90. 197. 150.  58. 100. 104.  99. 399.  90.\n 149. 110. 405. 175.  99. 199. 109.  79. 155. 700.  55.  95.  97.  65.\n 115.  95. 175. 110.  70. 160. 126.  80. 320.  80. 450. 150.  68.  99.\n 109. 100.  89. 105.  88. 250.  65.  99.  65.  40.  60.  67. 135.  75.\n 175.  65. 130.  53.  65.  93.  75. 115.  85.  55. 139. 135. 375. 115.\n  40.  90. 224. 130. 140.  65. 149. 195.  84.  99. 225.  70. 285. 200.\n 250.  69.  85. 300. 125.  40. 129.  65. 115. 225. 129. 155. 100.  40.\n  95. 110. 100. 150. 170.  40. 225.  92.  75.  40. 250. 158.  99.  95.\n 115. 175.  78.  69.  45. 250.  95. 100. 159. 120.  81.  45.  39. 114.\n  40. 200. 100. 130. 110. 245. 240.  99.  65. 174. 175. 135.  90. 175.\n  65. 239.  63. 190.  52.  85. 190. 117.  65. 125.  75. 159. 125.  55.\n 119.  68. 115. 100.  70. 175. 210.  29. 129. 160. 125. 130.  89.  50.\n  95. 350.  75.  95.  35. 120. 350.  55.  91. 200. 275.  98.  90. 125.\n  90. 229. 150.  65. 170. 110.  41. 100. 275. 145. 115.  80. 115. 105.\n 195.  99.  64. 100. 239.  90. 108.  50.  68.  99. 175.  29. 126. 125.\n  80. 150.  75. 195.  68. 120.  58. 168.  95. 300. 110.  37. 149.  80.\n 100.  80.  42. 125.  70. 349.  43. 183.  39. 100. 119.  80.  88.  95.\n 140. 375. 149. 137.  47. 145.  80. 139.  70. 145. 130. 109.  39. 157.\n 110.  80.  85.  60. 130. 200.  49. 156. 150.  40.  50.  85.  59. 165.\n 150. 170.  95. 115.  88. 300.  49.  50. 166. 125.  65. 139.  49.  85.\n 110.  28. 125. 109. 150.  65. 399. 120. 100. 140. 120. 129.  75.  80.\n  55. 140. 139.  85. 280. 115.  95. 225. 220.  90. 200.  90.  65.  75.\n  75.  85.  90. 115. 257.  98. 165. 175.  90.  60. 165. 175. 175.  89.\n  95. 215.  68.  75. 175. 150. 115. 170. 139.  95. 150.  45. 178.  85.\n 119. 120.  69. 175. 115.  68.  75.  75. 105.  75.  45. 125. 115.  75.\n 125.  52.  35. 250. 119. 100. 199.  85.  60. 110. 129.  87.  69. 250.\n  65. 150.  99.  75. 130. 135.  66.  50. 162.  50.  85. 250.  50. 200.\n  57. 145. 150.  80. 195.  70. 139. 240.  59.  95.  45. 209. 100. 108.\n  56.  80. 225. 195. 100.  49. 219.  95.  59. 180. 445.  90.  45.  30.\n  85. 120. 137. 141.  50. 250. 335. 159. 109. 350. 115. 110.  89. 118.\n 149. 129. 151. 360. 170.  96.  90. 175. 200. 149. 125.  50. 129.  90.\n 338.  97.  65. 330.  55. 100.  49.  41. 150. 550. 175. 175. 169. 175.\n  65. 150.  86.  53. 500. 129. 325.  55.  45. 300. 155. 245. 439. 115.\n 119.  40. 150. 200.  95. 215.  75.  75. 108. 136.  59.  75.  65.  90.\n 120.  40. 175. 129.  55. 117.  95. 110.  65. 174. 150. 200.  99.  99.\n  55.  90. 117. 105. 100. 200.  90.  80.  68. 144.  85.  85. 111.  95.\n 150.  38. 110. 350. 150.  70. 105.  71.  49. 100.  37. 215.  40. 120.\n 325. 139.  80.  70. 139. 119.  38. 132. 110.  85. 135. 129.  80. 300.\n 120. 250.  69.  62. 120.  66.  69.  35. 119.  90.  39.  65. 100. 100.\n  40.  65.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "regr = AdaBoostRegressor(random_state=0)\n",
    "regr.fit(X_train, y_train)\n",
    "y_ada_pred = regr.predict(X_test)\n",
    "regr.score(y_test,y_ada_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in normed_train_data.columns.tolist():\n",
    "    if normed_train_data[col].isna().any():\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>host_since</th>\n",
       "      <th>host_response_rate</th>\n",
       "      <th>host_acceptance_rate</th>\n",
       "      <th>host_total_listings_count</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>accommodates</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>beds</th>\n",
       "      <th>...</th>\n",
       "      <th>last_review</th>\n",
       "      <th>review_scores_rating</th>\n",
       "      <th>review_scores_accuracy</th>\n",
       "      <th>review_scores_cleanliness</th>\n",
       "      <th>review_scores_checkin</th>\n",
       "      <th>review_scores_communication</th>\n",
       "      <th>review_scores_location</th>\n",
       "      <th>review_scores_value</th>\n",
       "      <th>calculated_host_listings_count</th>\n",
       "      <th>reviews_per_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.327065</td>\n",
       "      <td>4.485599e-01</td>\n",
       "      <td>2.103086e-02</td>\n",
       "      <td>-0.230074</td>\n",
       "      <td>2.021838</td>\n",
       "      <td>-1.062650</td>\n",
       "      <td>0.303695</td>\n",
       "      <td>-0.438445</td>\n",
       "      <td>-0.353323</td>\n",
       "      <td>0.220012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.497284</td>\n",
       "      <td>0.828255</td>\n",
       "      <td>0.525314</td>\n",
       "      <td>0.559720</td>\n",
       "      <td>0.360738</td>\n",
       "      <td>0.337869</td>\n",
       "      <td>0.623998</td>\n",
       "      <td>0.733511</td>\n",
       "      <td>-0.333607</td>\n",
       "      <td>0.995673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.262966</td>\n",
       "      <td>-1.881057e+00</td>\n",
       "      <td>2.103086e-02</td>\n",
       "      <td>-0.230074</td>\n",
       "      <td>-0.968795</td>\n",
       "      <td>-1.490504</td>\n",
       "      <td>1.305126</td>\n",
       "      <td>2.080944</td>\n",
       "      <td>0.785765</td>\n",
       "      <td>1.084461</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.449784</td>\n",
       "      <td>0.374495</td>\n",
       "      <td>0.525314</td>\n",
       "      <td>-0.696443</td>\n",
       "      <td>0.360738</td>\n",
       "      <td>0.337869</td>\n",
       "      <td>0.623998</td>\n",
       "      <td>0.733511</td>\n",
       "      <td>-0.333607</td>\n",
       "      <td>-0.426506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.223794</td>\n",
       "      <td>1.324234e-15</td>\n",
       "      <td>7.988703e-15</td>\n",
       "      <td>-0.116083</td>\n",
       "      <td>0.754214</td>\n",
       "      <td>-0.325927</td>\n",
       "      <td>-1.198452</td>\n",
       "      <td>-0.438445</td>\n",
       "      <td>-0.353323</td>\n",
       "      <td>-0.644437</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.290464</td>\n",
       "      <td>0.828255</td>\n",
       "      <td>0.525314</td>\n",
       "      <td>0.559720</td>\n",
       "      <td>0.360738</td>\n",
       "      <td>0.337869</td>\n",
       "      <td>0.623998</td>\n",
       "      <td>0.733511</td>\n",
       "      <td>-0.333607</td>\n",
       "      <td>-1.027511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.958496</td>\n",
       "      <td>2.621906e-01</td>\n",
       "      <td>2.103086e-02</td>\n",
       "      <td>0.529862</td>\n",
       "      <td>0.879789</td>\n",
       "      <td>0.524697</td>\n",
       "      <td>-1.198452</td>\n",
       "      <td>-0.438445</td>\n",
       "      <td>-0.353323</td>\n",
       "      <td>-0.644437</td>\n",
       "      <td>...</td>\n",
       "      <td>0.576944</td>\n",
       "      <td>-0.835531</td>\n",
       "      <td>-0.909779</td>\n",
       "      <td>-0.696443</td>\n",
       "      <td>-1.320915</td>\n",
       "      <td>-1.421632</td>\n",
       "      <td>-0.969589</td>\n",
       "      <td>-0.602085</td>\n",
       "      <td>3.019978</td>\n",
       "      <td>1.781144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.518706</td>\n",
       "      <td>1.324234e-15</td>\n",
       "      <td>7.988703e-15</td>\n",
       "      <td>-0.230074</td>\n",
       "      <td>-0.363930</td>\n",
       "      <td>0.295008</td>\n",
       "      <td>-0.697737</td>\n",
       "      <td>-0.438445</td>\n",
       "      <td>-1.492411</td>\n",
       "      <td>-0.644437</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045878</td>\n",
       "      <td>-2.499317</td>\n",
       "      <td>-0.909779</td>\n",
       "      <td>-1.952605</td>\n",
       "      <td>-1.320915</td>\n",
       "      <td>-3.181132</td>\n",
       "      <td>-0.969589</td>\n",
       "      <td>-1.937680</td>\n",
       "      <td>-0.333607</td>\n",
       "      <td>-0.075424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2522</th>\n",
       "      <td>1.473068</td>\n",
       "      <td>-4.832867e-01</td>\n",
       "      <td>2.103086e-02</td>\n",
       "      <td>-0.230074</td>\n",
       "      <td>-1.514035</td>\n",
       "      <td>-0.934744</td>\n",
       "      <td>-0.697737</td>\n",
       "      <td>-0.438445</td>\n",
       "      <td>-0.353323</td>\n",
       "      <td>-0.644437</td>\n",
       "      <td>...</td>\n",
       "      <td>0.497284</td>\n",
       "      <td>0.828255</td>\n",
       "      <td>0.525314</td>\n",
       "      <td>0.559720</td>\n",
       "      <td>0.360738</td>\n",
       "      <td>0.337869</td>\n",
       "      <td>0.623998</td>\n",
       "      <td>0.733511</td>\n",
       "      <td>-0.333607</td>\n",
       "      <td>-0.004018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2523</th>\n",
       "      <td>1.341309</td>\n",
       "      <td>4.485599e-01</td>\n",
       "      <td>2.103086e-02</td>\n",
       "      <td>-0.230074</td>\n",
       "      <td>-1.283435</td>\n",
       "      <td>-2.598542</td>\n",
       "      <td>-0.697737</td>\n",
       "      <td>-0.438445</td>\n",
       "      <td>-0.353323</td>\n",
       "      <td>-0.644437</td>\n",
       "      <td>...</td>\n",
       "      <td>0.630051</td>\n",
       "      <td>0.828255</td>\n",
       "      <td>0.525314</td>\n",
       "      <td>0.559720</td>\n",
       "      <td>0.360738</td>\n",
       "      <td>0.337869</td>\n",
       "      <td>0.623998</td>\n",
       "      <td>0.733511</td>\n",
       "      <td>-0.333607</td>\n",
       "      <td>-0.099227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2524</th>\n",
       "      <td>1.019034</td>\n",
       "      <td>4.485599e-01</td>\n",
       "      <td>2.103086e-02</td>\n",
       "      <td>-0.192077</td>\n",
       "      <td>-2.538499</td>\n",
       "      <td>-0.849491</td>\n",
       "      <td>-0.197021</td>\n",
       "      <td>-0.438445</td>\n",
       "      <td>-0.353323</td>\n",
       "      <td>-0.644437</td>\n",
       "      <td>...</td>\n",
       "      <td>0.231751</td>\n",
       "      <td>0.828255</td>\n",
       "      <td>0.525314</td>\n",
       "      <td>0.559720</td>\n",
       "      <td>0.360738</td>\n",
       "      <td>0.337869</td>\n",
       "      <td>0.623998</td>\n",
       "      <td>0.733511</td>\n",
       "      <td>-0.165928</td>\n",
       "      <td>-0.670478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2525</th>\n",
       "      <td>0.053988</td>\n",
       "      <td>4.485599e-01</td>\n",
       "      <td>2.103086e-02</td>\n",
       "      <td>-0.230074</td>\n",
       "      <td>-0.514914</td>\n",
       "      <td>0.222120</td>\n",
       "      <td>-0.697737</td>\n",
       "      <td>-0.438445</td>\n",
       "      <td>-0.353323</td>\n",
       "      <td>-0.644437</td>\n",
       "      <td>...</td>\n",
       "      <td>0.523838</td>\n",
       "      <td>0.374495</td>\n",
       "      <td>0.525314</td>\n",
       "      <td>0.559720</td>\n",
       "      <td>0.360738</td>\n",
       "      <td>0.337869</td>\n",
       "      <td>0.623998</td>\n",
       "      <td>0.733511</td>\n",
       "      <td>-0.333607</td>\n",
       "      <td>2.471406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2526</th>\n",
       "      <td>0.370922</td>\n",
       "      <td>-4.210673e+00</td>\n",
       "      <td>2.103086e-02</td>\n",
       "      <td>-0.230074</td>\n",
       "      <td>-0.804044</td>\n",
       "      <td>1.218943</td>\n",
       "      <td>1.305126</td>\n",
       "      <td>0.401352</td>\n",
       "      <td>1.924852</td>\n",
       "      <td>1.084461</td>\n",
       "      <td>...</td>\n",
       "      <td>0.373369</td>\n",
       "      <td>-0.079265</td>\n",
       "      <td>0.525314</td>\n",
       "      <td>0.559720</td>\n",
       "      <td>0.360738</td>\n",
       "      <td>-1.421632</td>\n",
       "      <td>-0.969589</td>\n",
       "      <td>0.733511</td>\n",
       "      <td>-0.333607</td>\n",
       "      <td>-0.420556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2527 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      host_since  host_response_rate  host_acceptance_rate  \\\n",
       "0       1.327065        4.485599e-01          2.103086e-02   \n",
       "1       1.262966       -1.881057e+00          2.103086e-02   \n",
       "2       1.223794        1.324234e-15          7.988703e-15   \n",
       "3       0.958496        2.621906e-01          2.103086e-02   \n",
       "4       0.518706        1.324234e-15          7.988703e-15   \n",
       "...          ...                 ...                   ...   \n",
       "2522    1.473068       -4.832867e-01          2.103086e-02   \n",
       "2523    1.341309        4.485599e-01          2.103086e-02   \n",
       "2524    1.019034        4.485599e-01          2.103086e-02   \n",
       "2525    0.053988        4.485599e-01          2.103086e-02   \n",
       "2526    0.370922       -4.210673e+00          2.103086e-02   \n",
       "\n",
       "      host_total_listings_count  latitude  longitude  accommodates  bathrooms  \\\n",
       "0                     -0.230074  2.021838  -1.062650      0.303695  -0.438445   \n",
       "1                     -0.230074 -0.968795  -1.490504      1.305126   2.080944   \n",
       "2                     -0.116083  0.754214  -0.325927     -1.198452  -0.438445   \n",
       "3                      0.529862  0.879789   0.524697     -1.198452  -0.438445   \n",
       "4                     -0.230074 -0.363930   0.295008     -0.697737  -0.438445   \n",
       "...                         ...       ...        ...           ...        ...   \n",
       "2522                  -0.230074 -1.514035  -0.934744     -0.697737  -0.438445   \n",
       "2523                  -0.230074 -1.283435  -2.598542     -0.697737  -0.438445   \n",
       "2524                  -0.192077 -2.538499  -0.849491     -0.197021  -0.438445   \n",
       "2525                  -0.230074 -0.514914   0.222120     -0.697737  -0.438445   \n",
       "2526                  -0.230074 -0.804044   1.218943      1.305126   0.401352   \n",
       "\n",
       "      bedrooms      beds  ...  last_review  review_scores_rating  \\\n",
       "0    -0.353323  0.220012  ...     0.497284              0.828255   \n",
       "1     0.785765  1.084461  ...    -0.449784              0.374495   \n",
       "2    -0.353323 -0.644437  ...    -0.290464              0.828255   \n",
       "3    -0.353323 -0.644437  ...     0.576944             -0.835531   \n",
       "4    -1.492411 -0.644437  ...     0.045878             -2.499317   \n",
       "...        ...       ...  ...          ...                   ...   \n",
       "2522 -0.353323 -0.644437  ...     0.497284              0.828255   \n",
       "2523 -0.353323 -0.644437  ...     0.630051              0.828255   \n",
       "2524 -0.353323 -0.644437  ...     0.231751              0.828255   \n",
       "2525 -0.353323 -0.644437  ...     0.523838              0.374495   \n",
       "2526  1.924852  1.084461  ...     0.373369             -0.079265   \n",
       "\n",
       "      review_scores_accuracy  review_scores_cleanliness  \\\n",
       "0                   0.525314                   0.559720   \n",
       "1                   0.525314                  -0.696443   \n",
       "2                   0.525314                   0.559720   \n",
       "3                  -0.909779                  -0.696443   \n",
       "4                  -0.909779                  -1.952605   \n",
       "...                      ...                        ...   \n",
       "2522                0.525314                   0.559720   \n",
       "2523                0.525314                   0.559720   \n",
       "2524                0.525314                   0.559720   \n",
       "2525                0.525314                   0.559720   \n",
       "2526                0.525314                   0.559720   \n",
       "\n",
       "      review_scores_checkin  review_scores_communication  \\\n",
       "0                  0.360738                     0.337869   \n",
       "1                  0.360738                     0.337869   \n",
       "2                  0.360738                     0.337869   \n",
       "3                 -1.320915                    -1.421632   \n",
       "4                 -1.320915                    -3.181132   \n",
       "...                     ...                          ...   \n",
       "2522               0.360738                     0.337869   \n",
       "2523               0.360738                     0.337869   \n",
       "2524               0.360738                     0.337869   \n",
       "2525               0.360738                     0.337869   \n",
       "2526               0.360738                    -1.421632   \n",
       "\n",
       "      review_scores_location  review_scores_value  \\\n",
       "0                   0.623998             0.733511   \n",
       "1                   0.623998             0.733511   \n",
       "2                   0.623998             0.733511   \n",
       "3                  -0.969589            -0.602085   \n",
       "4                  -0.969589            -1.937680   \n",
       "...                      ...                  ...   \n",
       "2522                0.623998             0.733511   \n",
       "2523                0.623998             0.733511   \n",
       "2524                0.623998             0.733511   \n",
       "2525                0.623998             0.733511   \n",
       "2526               -0.969589             0.733511   \n",
       "\n",
       "      calculated_host_listings_count  reviews_per_month  \n",
       "0                          -0.333607           0.995673  \n",
       "1                          -0.333607          -0.426506  \n",
       "2                          -0.333607          -1.027511  \n",
       "3                           3.019978           1.781144  \n",
       "4                          -0.333607          -0.075424  \n",
       "...                              ...                ...  \n",
       "2522                       -0.333607          -0.004018  \n",
       "2523                       -0.333607          -0.099227  \n",
       "2524                       -0.165928          -0.670478  \n",
       "2525                       -0.333607           2.471406  \n",
       "2526                       -0.333607          -0.420556  \n",
       "\n",
       "[2527 rows x 34 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train2= pd.DataFrame(data=X_train,columns=X_columns)\n",
    "X_train2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 2021 samples, validate on 506 samples\n",
      "Epoch 1/50\n",
      "2021/2021 - 0s - loss: 24252.8223 - accuracy: 0.0000e+00 - val_loss: 21093.4818 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/50\n",
      "2021/2021 - 0s - loss: 19884.8513 - accuracy: 0.0000e+00 - val_loss: 15397.0534 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/50\n",
      "2021/2021 - 0s - loss: 12151.2607 - accuracy: 0.0000e+00 - val_loss: 7610.4523 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/50\n",
      "2021/2021 - 0s - loss: 5842.0853 - accuracy: 0.0000e+00 - val_loss: 5336.3702 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/50\n",
      "2021/2021 - 0s - loss: 5004.6235 - accuracy: 0.0000e+00 - val_loss: 5326.7561 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/50\n",
      "2021/2021 - 0s - loss: 4723.8133 - accuracy: 0.0000e+00 - val_loss: 5216.8753 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/50\n",
      "2021/2021 - 0s - loss: 4534.7712 - accuracy: 0.0000e+00 - val_loss: 5268.5714 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/50\n",
      "2021/2021 - 0s - loss: 4403.1729 - accuracy: 0.0000e+00 - val_loss: 5316.6331 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/50\n",
      "2021/2021 - 0s - loss: 4288.0250 - accuracy: 0.0000e+00 - val_loss: 5327.4754 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/50\n",
      "2021/2021 - 0s - loss: 4198.9909 - accuracy: 0.0000e+00 - val_loss: 5409.6016 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/50\n",
      "2021/2021 - 0s - loss: 4143.2622 - accuracy: 0.0000e+00 - val_loss: 5368.5208 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x18790f99508>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 100\n",
    "hidden_layer_size = 200\n",
    "NUM_EPOCHS = 50\n",
    "output_size = 1\n",
    "\n",
    "model = tf.keras.Sequential([    \n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu', input_shape=[len(X_train2.keys())]),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    \n",
    "    tf.keras.layers.Dense(output_size, activation='relu') # output layer\n",
    "])\n",
    "\n",
    "\"\"\" \n",
    "Now we have to choose the optimizer and the loss function.\n",
    "\n",
    "metrics will show us what we are interested in obtaining at each iteration.\n",
    "\"\"\"\n",
    "model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=5)\n",
    "\n",
    "model.fit(X_train2, \n",
    "          y_train, \n",
    "          batch_size = BATCH_SIZE, \n",
    "          epochs = NUM_EPOCHS, \n",
    "          callbacks = [early_stopping],\n",
    "          validation_split = 0.2,\n",
    "          verbose =2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3-TF2.0]",
   "language": "python",
   "name": "conda-env-py3-TF2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
