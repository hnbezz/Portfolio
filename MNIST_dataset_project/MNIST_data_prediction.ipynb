{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing lybraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here I'll just put all the hyperparameters so when I'm tweeking the model, it is easier.\n",
    "\"\"\"\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 200\n",
    "hidden_layer_size = 600\n",
    "NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True, shuffle_files=True)\n",
    "\n",
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the mnist dataset does not have a specifically separated validation dataset, we have to do by ourselves. Let's start with selecting the number of samples for validation and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I separate 10% od the training dataset to be validation samples\n",
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to scale the data so all input values are between 0 and 1, so we will define a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255.\n",
    "    return image, label\n",
    "\n",
    "scaled_train_and_validation_data = mnist_train.map(scale)\n",
    "\n",
    "test_data = mnist_test.map(scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to shuffle the data so when we batch it, there is no pattern that will harm our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This buffer size parameter is used in cases when we are dealing with enormous datasets.\n",
    "In such cases we can't shuffle the whole data set in one go because we can't possibly fit it all in the memory of the computer.\n",
    "So instead we must instruct tensor flow to take samples ten thousand at a time shuffle them and then \n",
    "take the next ten thousand. Note that if the buffer size is equal to 1, no shuffling will actually happen.\n",
    "So if the buffer size is equal or bigger than the total number of samples shuffling will take place at\n",
    "once and shuffle them uniformly.\"\"\"\n",
    "\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Now we can extract the train and validation data\"\"\"\n",
    "\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Now we want to set the batches. We must set a batch size and prepare our data for batching.\n",
    "Just a quick memory refresh, a batch size of one is the stochastic gradient descent, while a batch size equal \n",
    "to the number of samples equals the gradient descent we've seen until now.\n",
    "\n",
    "So we want a number relatively small with regard to the data set but reasonably high.\n",
    "So it would allow us to preserve the underlying dependencies\"\"\"\n",
    "\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "\n",
    "\"\"\"Well since we won't be backpropagating on the validation data but only forward propagating we don't really need\n",
    "to batch it. Remember that batching was useful in updating weights. However the model expects our validation set in the batch form \n",
    "too. That's why we should overwrite validation data with:\"\"\"\n",
    "\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "\n",
    "\"\"\"As the test data we also just want to forward propagate, we only need to make into a batch with size num_test_samples\"\"\"\n",
    "test_data = test_data.batch(num_test_samples)\n",
    "\n",
    "\n",
    "validation_inputs, validation_targets = next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784 # 28x28 pixels\n",
    "output_size = 10 # due to the 10 digits\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    \n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28, 1)), # input layer\n",
    "    \n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer\n",
    "    \n",
    "    # the final layer is no different, we just make sure to activate it with softmax\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax') # output layer\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to choose the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "There are several different optimizers, loss functions and metrics. One should be VERY careful when choosing them.\n",
    "\"\"\"\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "270/270 - 22s - loss: 0.0163 - accuracy: 0.9948 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "270/270 - 21s - loss: 0.0095 - accuracy: 0.9969 - val_loss: 0.0089 - val_accuracy: 0.9977\n",
      "Epoch 3/10\n",
      "270/270 - 20s - loss: 0.0102 - accuracy: 0.9964 - val_loss: 0.0098 - val_accuracy: 0.9962\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x19b896cf908>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VALIDATION_STEPS = num_validation_samples\n",
    "\n",
    "\"\"\"\n",
    "early_stopping will check when the validation loss will increase and stop it. patience is a argument that will allow to skip the \n",
    "first 'n' epochs that has an increase in val_loss.\n",
    "\"\"\"\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(patience=1)\n",
    "\n",
    "model.fit(train_data,\n",
    "          epochs = NUM_EPOCHS,\n",
    "          callbacks = [early_stop],\n",
    "          validation_data=(validation_inputs, validation_targets), \n",
    "          validation_steps=VALIDATION_STEPS,\n",
    "          verbose =2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step - loss: 0.0847 - accuracy: 0.9811\n",
      "Test loss: 0.08. Test accuracy: 98.11%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)\n",
    "\n",
    "print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3-TF2.0]",
   "language": "python",
   "name": "conda-env-py3-TF2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
