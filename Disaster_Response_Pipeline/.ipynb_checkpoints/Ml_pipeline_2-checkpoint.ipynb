{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hnbez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hnbez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report, accuracy_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.utils import resample\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "nltk.download(['punkt', 'stopwords'])\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///db_tweet_disasters.db')\n",
    "df = pd.read_sql('SELECT * FROM message_categories', engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "stop_words = stopwords.words('english')\n",
    "def tokenize2(text):\n",
    "    detected_urls = re.findall(url_regex, text)\n",
    "    for url in detected_urls:\n",
    "        text = text.replace(url, \"urlplaceholder\")\n",
    "    \n",
    "    # remove punctuation\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [w for w in tokens if w not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    \n",
    "    stemmed_tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
    "    \n",
    "    return stemmed_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_pred_rec(y_test,y_pred):\n",
    "    accuracy = (y_test == y_pred).mean()\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recalls = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1_scores = f1_score(y_test, y_pred, average='weighted')\n",
    "    print('the accuracy is ' + str(accuracy))\n",
    "    print('the precision is ' + str(precision))\n",
    "    print('the recall is ' + str(recalls))\n",
    "    print('the f1_score is ' + str(f1_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metricss(test_labels, predicted_labels, col_names):\n",
    "    metrics = []\n",
    "    for i in range(len(col_names)):\n",
    "        sum1s = sum(test_labels.iloc[:, i])\n",
    "        sum0s = len(test_labels) - sum1s\n",
    "        accuracy = accuracy_score(test_labels.iloc[:, i], predicted_labels.iloc[:, i])\n",
    "        precision = precision_score(test_labels.iloc[:, i], predicted_labels.iloc[:, i])\n",
    "        recall = recall_score(test_labels.iloc[:, i], predicted_labels.iloc[:, i])\n",
    "        f1 = f1_score(test_labels.iloc[:, i], predicted_labels.iloc[:, i])\n",
    "        \n",
    "        metrics.append([sum1s, sum0s, accuracy, precision, recall, f1])\n",
    "    \n",
    "    col_names.append('mean')\n",
    "    \n",
    "    metrics = np.array(metrics)\n",
    "    metrics_df = pd.DataFrame(data = metrics, columns = ['# 1s', '# 0s', 'Accuracy', 'Precision', 'Recall', 'F1'])\n",
    "    metrics_df = metrics_df.append({'# 1s': sum(metrics[:,0]/len(metrics[:,0])), \n",
    "                                    '# 0s': sum(metrics[:,1]/len(metrics[:,1])), \n",
    "                                    'Accuracy': sum(metrics[:,2]/len(metrics[:,2])),\n",
    "                                    'Precision': sum(metrics[:,3]/len(metrics[:,3])),\n",
    "                                    'Recall': sum(metrics[:,4]/len(metrics[:,4])),\n",
    "                                    'F1': sum(metrics[:,5]/len(metrics[:,5]))}, ignore_index=True)\n",
    "    \n",
    "    metrics_df.index = col_names\n",
    "    \n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding custom estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StartVerbExtractor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "\n",
    "    def start_verb(self, text):\n",
    "        sentence_list = nltk.sent_tokenize(text)\n",
    "        for sentence in sentence_list:\n",
    "            pos_tags = nltk.pos_tag(tokenize(sentence))\n",
    "            if len(pos_tags) != 0:\n",
    "                first_word, first_tag = pos_tags[0]\n",
    "                if first_tag in ['VB', 'VBP'] or first_word == 'RT':\n",
    "                    return 1\n",
    "        return 0\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "\n",
    "    def transform(self, X):\n",
    "        X_tag = pd.Series(X).apply(self.start_verb)\n",
    "        return pd.DataFrame(X_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_len(data):\n",
    "    return np.array([len(text) for text in data]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting into training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['message']\n",
    "y = df.drop(['message','related_alone','offer','request'],axis=1)\n",
    "y_cols = y.columns.tolist()\n",
    "\n",
    "# Here we separate into stratified training and test datasets\n",
    "msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.25, random_state=42)\n",
    "\n",
    "for train_index, test_index in msss.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y.values[train_index], y.values[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.DataFrame(y_train,columns=y_cols)\n",
    "y_test = pd.DataFrame(y_test,columns=y_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>related</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>medical_help</th>\n",
       "      <th>medical_products</th>\n",
       "      <th>search_and_rescue</th>\n",
       "      <th>security</th>\n",
       "      <th>military</th>\n",
       "      <th>water</th>\n",
       "      <th>food</th>\n",
       "      <th>shelter</th>\n",
       "      <th>...</th>\n",
       "      <th>aid_centers</th>\n",
       "      <th>other_infrastructure</th>\n",
       "      <th>weather_related</th>\n",
       "      <th>floods</th>\n",
       "      <th>storm</th>\n",
       "      <th>fire</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>cold</th>\n",
       "      <th>other_weather</th>\n",
       "      <th>direct_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19516</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19517</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19518</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19519</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19520</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19521 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       related  aid_related  medical_help  medical_products  \\\n",
       "0            1            0             0                 0   \n",
       "1            1            1             0                 0   \n",
       "2            1            0             0                 0   \n",
       "3            1            1             0                 1   \n",
       "4            1            0             0                 0   \n",
       "...        ...          ...           ...               ...   \n",
       "19516        1            1             1                 1   \n",
       "19517        1            1             0                 0   \n",
       "19518        1            0             0                 0   \n",
       "19519        0            0             0                 0   \n",
       "19520        1            0             0                 0   \n",
       "\n",
       "       search_and_rescue  security  military  water  food  shelter  ...  \\\n",
       "0                      0         0         0      0     0        0  ...   \n",
       "1                      0         0         0      0     0        0  ...   \n",
       "2                      0         0         0      0     0        0  ...   \n",
       "3                      0         0         0      0     0        0  ...   \n",
       "4                      0         0         0      0     0        0  ...   \n",
       "...                  ...       ...       ...    ...   ...      ...  ...   \n",
       "19516                  0         0         0      0     0        0  ...   \n",
       "19517                  0         0         0      0     0        0  ...   \n",
       "19518                  0         0         0      0     0        0  ...   \n",
       "19519                  0         0         0      0     0        0  ...   \n",
       "19520                  0         0         0      0     0        0  ...   \n",
       "\n",
       "       aid_centers  other_infrastructure  weather_related  floods  storm  \\\n",
       "0                0                     0                0       0      0   \n",
       "1                0                     0                1       0      1   \n",
       "2                0                     0                0       0      0   \n",
       "3                0                     0                0       0      0   \n",
       "4                0                     0                0       0      0   \n",
       "...            ...                   ...              ...     ...    ...   \n",
       "19516            0                     0                0       0      0   \n",
       "19517            0                     0                0       0      0   \n",
       "19518            0                     0                0       0      0   \n",
       "19519            0                     0                0       0      0   \n",
       "19520            0                     0                0       0      0   \n",
       "\n",
       "       fire  earthquake  cold  other_weather  direct_report  \n",
       "0         0           0     0              0              0  \n",
       "1         0           0     0              0              0  \n",
       "2         0           0     0              0              0  \n",
       "3         0           0     0              0              0  \n",
       "4         0           0     0              0              0  \n",
       "...     ...         ...   ...            ...            ...  \n",
       "19516     0           0     0              0              0  \n",
       "19517     0           0     0              0              0  \n",
       "19518     0           0     0              0              1  \n",
       "19519     0           0     0              0              0  \n",
       "19520     0           0     0              0              0  \n",
       "\n",
       "[19521 rows x 33 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model with adaboost with StartVerbExtractor\n",
    "\n",
    "It is a little worst than adaboost with StartVebExtractor and text_len, so I will use that for gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Adaboost_pipeline():\n",
    "    pipeline = Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "\n",
    "            ('text_pipeline', Pipeline([\n",
    "                ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "                ('tfidf', TfidfTransformer())\n",
    "            ])),\n",
    "\n",
    "            ('start_verb', StartVerbExtractor())\n",
    "        ])),\n",
    "\n",
    "        ('clf', MultiOutputClassifier(AdaBoostClassifier()))\n",
    "    ])\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_ada = Adaboost_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.seed(42)\n",
    "#model_ada.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred_ada = model_ada.predict(X_test)\n",
    "#y_pred_ada = pd.DataFrame(y_pred_ada, columns=y_test.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#metricss(y_test, y_pred_ada, y_test.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_msg = ['There is a fire in building nearby and there are people in it.']\n",
    "#test = model_ada.predict(test_msg)\n",
    "#print(y_train.columns.values[(test.flatten()==1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model with adaboost, StartVerbExtractor and text_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Adaboost_len_pipeline():\n",
    "    pipeline = Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "\n",
    "            ('text_pipeline', Pipeline([\n",
    "                ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "                ('tfidf', TfidfTransformer())\n",
    "            ])),\n",
    "\n",
    "            ('start_verb', StartVerbExtractor()),\n",
    "            ('length', Pipeline([('text_length', FunctionTransformer(get_text_len, validate=False))]))\n",
    "        ])),\n",
    "\n",
    "        ('clf', MultiOutputClassifier(AdaBoostClassifier()))\n",
    "    ])\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ada_len = Adaboost_len_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('features',\n",
       "                 FeatureUnion(n_jobs=None,\n",
       "                              transformer_list=[('text_pipeline',\n",
       "                                                 Pipeline(memory=None,\n",
       "                                                          steps=[('vect',\n",
       "                                                                  CountVectorizer(analyzer='word',\n",
       "                                                                                  binary=False,\n",
       "                                                                                  decode_error='strict',\n",
       "                                                                                  dtype=<class 'numpy.int64'>,\n",
       "                                                                                  encoding='utf-8',\n",
       "                                                                                  input='content',\n",
       "                                                                                  lowercase=True,\n",
       "                                                                                  max_df=1.0,\n",
       "                                                                                  max_features=None,\n",
       "                                                                                  min_df=1,\n",
       "                                                                                  ngram_range=(1,\n",
       "                                                                                               1),\n",
       "                                                                                  preprocessor=Non...\n",
       "                                                                                      func=<function get_text_len at 0x000001C48E780798>,\n",
       "                                                                                      inv_kw_args=None,\n",
       "                                                                                      inverse_func=None,\n",
       "                                                                                      kw_args=None,\n",
       "                                                                                      validate=False))],\n",
       "                                                          verbose=False))],\n",
       "                              transformer_weights=None, verbose=False)),\n",
       "                ('clf',\n",
       "                 MultiOutputClassifier(estimator=AdaBoostClassifier(algorithm='SAMME.R',\n",
       "                                                                    base_estimator=None,\n",
       "                                                                    learning_rate=1.0,\n",
       "                                                                    n_estimators=50,\n",
       "                                                                    random_state=None),\n",
       "                                       n_jobs=None))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "model_ada_len.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ada_len = model_ada_len.predict(X_test)\n",
    "y_pred_ada_len = pd.DataFrame(y_pred_ada_len, columns=y_test.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># 1s</th>\n",
       "      <th># 0s</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>related</th>\n",
       "      <td>4976.000000</td>\n",
       "      <td>1531.000000</td>\n",
       "      <td>0.806516</td>\n",
       "      <td>0.846929</td>\n",
       "      <td>0.911777</td>\n",
       "      <td>0.878157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aid_related</th>\n",
       "      <td>2715.000000</td>\n",
       "      <td>3792.000000</td>\n",
       "      <td>0.754572</td>\n",
       "      <td>0.749109</td>\n",
       "      <td>0.619153</td>\n",
       "      <td>0.677959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medical_help</th>\n",
       "      <td>521.000000</td>\n",
       "      <td>5986.000000</td>\n",
       "      <td>0.926080</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.287908</td>\n",
       "      <td>0.384123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medical_products</th>\n",
       "      <td>328.000000</td>\n",
       "      <td>6179.000000</td>\n",
       "      <td>0.955740</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.317073</td>\n",
       "      <td>0.419355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>search_and_rescue</th>\n",
       "      <td>181.000000</td>\n",
       "      <td>6326.000000</td>\n",
       "      <td>0.974489</td>\n",
       "      <td>0.659574</td>\n",
       "      <td>0.171271</td>\n",
       "      <td>0.271930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>security</th>\n",
       "      <td>118.000000</td>\n",
       "      <td>6389.000000</td>\n",
       "      <td>0.980329</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.072464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>military</th>\n",
       "      <td>215.000000</td>\n",
       "      <td>6292.000000</td>\n",
       "      <td>0.970801</td>\n",
       "      <td>0.606838</td>\n",
       "      <td>0.330233</td>\n",
       "      <td>0.427711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>water</th>\n",
       "      <td>418.000000</td>\n",
       "      <td>6089.000000</td>\n",
       "      <td>0.960197</td>\n",
       "      <td>0.710875</td>\n",
       "      <td>0.641148</td>\n",
       "      <td>0.674214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>food</th>\n",
       "      <td>731.000000</td>\n",
       "      <td>5776.000000</td>\n",
       "      <td>0.944060</td>\n",
       "      <td>0.796446</td>\n",
       "      <td>0.674419</td>\n",
       "      <td>0.730370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shelter</th>\n",
       "      <td>578.000000</td>\n",
       "      <td>5929.000000</td>\n",
       "      <td>0.945136</td>\n",
       "      <td>0.776942</td>\n",
       "      <td>0.536332</td>\n",
       "      <td>0.634596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clothing</th>\n",
       "      <td>101.000000</td>\n",
       "      <td>6406.000000</td>\n",
       "      <td>0.988935</td>\n",
       "      <td>0.704225</td>\n",
       "      <td>0.495050</td>\n",
       "      <td>0.581395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>money</th>\n",
       "      <td>151.000000</td>\n",
       "      <td>6356.000000</td>\n",
       "      <td>0.977102</td>\n",
       "      <td>0.512821</td>\n",
       "      <td>0.264901</td>\n",
       "      <td>0.349345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>missing_people</th>\n",
       "      <td>74.000000</td>\n",
       "      <td>6433.000000</td>\n",
       "      <td>0.989242</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.175676</td>\n",
       "      <td>0.270833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>refugees</th>\n",
       "      <td>219.000000</td>\n",
       "      <td>6288.000000</td>\n",
       "      <td>0.968803</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.273973</td>\n",
       "      <td>0.371517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>death</th>\n",
       "      <td>298.000000</td>\n",
       "      <td>6209.000000</td>\n",
       "      <td>0.964807</td>\n",
       "      <td>0.660465</td>\n",
       "      <td>0.476510</td>\n",
       "      <td>0.553606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other_aid</th>\n",
       "      <td>859.000000</td>\n",
       "      <td>5648.000000</td>\n",
       "      <td>0.867988</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.157159</td>\n",
       "      <td>0.239150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>infrastructure_related</th>\n",
       "      <td>430.000000</td>\n",
       "      <td>6077.000000</td>\n",
       "      <td>0.929000</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.083721</td>\n",
       "      <td>0.134831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transport</th>\n",
       "      <td>300.000000</td>\n",
       "      <td>6207.000000</td>\n",
       "      <td>0.961272</td>\n",
       "      <td>0.735294</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.373134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>buildings</th>\n",
       "      <td>333.000000</td>\n",
       "      <td>6174.000000</td>\n",
       "      <td>0.959121</td>\n",
       "      <td>0.670051</td>\n",
       "      <td>0.396396</td>\n",
       "      <td>0.498113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>electricity</th>\n",
       "      <td>133.000000</td>\n",
       "      <td>6374.000000</td>\n",
       "      <td>0.980329</td>\n",
       "      <td>0.542373</td>\n",
       "      <td>0.240602</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tools</th>\n",
       "      <td>40.000000</td>\n",
       "      <td>6467.000000</td>\n",
       "      <td>0.993392</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.044444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hospitals</th>\n",
       "      <td>71.000000</td>\n",
       "      <td>6436.000000</td>\n",
       "      <td>0.987091</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.056338</td>\n",
       "      <td>0.086957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shops</th>\n",
       "      <td>30.000000</td>\n",
       "      <td>6477.000000</td>\n",
       "      <td>0.994160</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aid_centers</th>\n",
       "      <td>77.000000</td>\n",
       "      <td>6430.000000</td>\n",
       "      <td>0.986783</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.077922</td>\n",
       "      <td>0.122449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other_infrastructure</th>\n",
       "      <td>288.000000</td>\n",
       "      <td>6219.000000</td>\n",
       "      <td>0.949900</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.065972</td>\n",
       "      <td>0.104396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weather_related</th>\n",
       "      <td>1824.000000</td>\n",
       "      <td>4683.000000</td>\n",
       "      <td>0.877517</td>\n",
       "      <td>0.857342</td>\n",
       "      <td>0.675439</td>\n",
       "      <td>0.755596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>floods</th>\n",
       "      <td>539.000000</td>\n",
       "      <td>5968.000000</td>\n",
       "      <td>0.955433</td>\n",
       "      <td>0.839237</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.679912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>storm</th>\n",
       "      <td>611.000000</td>\n",
       "      <td>5896.000000</td>\n",
       "      <td>0.945751</td>\n",
       "      <td>0.747126</td>\n",
       "      <td>0.638298</td>\n",
       "      <td>0.688438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fire</th>\n",
       "      <td>70.000000</td>\n",
       "      <td>6437.000000</td>\n",
       "      <td>0.989242</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.242857</td>\n",
       "      <td>0.326923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>earthquake</th>\n",
       "      <td>610.000000</td>\n",
       "      <td>5897.000000</td>\n",
       "      <td>0.968957</td>\n",
       "      <td>0.865591</td>\n",
       "      <td>0.791803</td>\n",
       "      <td>0.827055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cold</th>\n",
       "      <td>132.000000</td>\n",
       "      <td>6375.000000</td>\n",
       "      <td>0.984478</td>\n",
       "      <td>0.731343</td>\n",
       "      <td>0.371212</td>\n",
       "      <td>0.492462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other_weather</th>\n",
       "      <td>344.000000</td>\n",
       "      <td>6163.000000</td>\n",
       "      <td>0.947288</td>\n",
       "      <td>0.504065</td>\n",
       "      <td>0.180233</td>\n",
       "      <td>0.265525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>direct_report</th>\n",
       "      <td>1269.000000</td>\n",
       "      <td>5238.000000</td>\n",
       "      <td>0.858306</td>\n",
       "      <td>0.731025</td>\n",
       "      <td>0.432624</td>\n",
       "      <td>0.543564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>593.454545</td>\n",
       "      <td>5913.545455</td>\n",
       "      <td>0.946752</td>\n",
       "      <td>0.579813</td>\n",
       "      <td>0.347721</td>\n",
       "      <td>0.418602</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               # 1s         # 0s  Accuracy  Precision  \\\n",
       "related                 4976.000000  1531.000000  0.806516   0.846929   \n",
       "aid_related             2715.000000  3792.000000  0.754572   0.749109   \n",
       "medical_help             521.000000  5986.000000  0.926080   0.576923   \n",
       "medical_products         328.000000  6179.000000  0.955740   0.619048   \n",
       "search_and_rescue        181.000000  6326.000000  0.974489   0.659574   \n",
       "security                 118.000000  6389.000000  0.980329   0.250000   \n",
       "military                 215.000000  6292.000000  0.970801   0.606838   \n",
       "water                    418.000000  6089.000000  0.960197   0.710875   \n",
       "food                     731.000000  5776.000000  0.944060   0.796446   \n",
       "shelter                  578.000000  5929.000000  0.945136   0.776942   \n",
       "clothing                 101.000000  6406.000000  0.988935   0.704225   \n",
       "money                    151.000000  6356.000000  0.977102   0.512821   \n",
       "missing_people            74.000000  6433.000000  0.989242   0.590909   \n",
       "refugees                 219.000000  6288.000000  0.968803   0.576923   \n",
       "death                    298.000000  6209.000000  0.964807   0.660465   \n",
       "other_aid                859.000000  5648.000000  0.867988   0.500000   \n",
       "infrastructure_related   430.000000  6077.000000  0.929000   0.346154   \n",
       "transport                300.000000  6207.000000  0.961272   0.735294   \n",
       "buildings                333.000000  6174.000000  0.959121   0.670051   \n",
       "electricity              133.000000  6374.000000  0.980329   0.542373   \n",
       "tools                     40.000000  6467.000000  0.993392   0.200000   \n",
       "hospitals                 71.000000  6436.000000  0.987091   0.190476   \n",
       "shops                     30.000000  6477.000000  0.994160   0.000000   \n",
       "aid_centers               77.000000  6430.000000  0.986783   0.285714   \n",
       "other_infrastructure     288.000000  6219.000000  0.949900   0.250000   \n",
       "weather_related         1824.000000  4683.000000  0.877517   0.857342   \n",
       "floods                   539.000000  5968.000000  0.955433   0.839237   \n",
       "storm                    611.000000  5896.000000  0.945751   0.747126   \n",
       "fire                      70.000000  6437.000000  0.989242   0.500000   \n",
       "earthquake               610.000000  5897.000000  0.968957   0.865591   \n",
       "cold                     132.000000  6375.000000  0.984478   0.731343   \n",
       "other_weather            344.000000  6163.000000  0.947288   0.504065   \n",
       "direct_report           1269.000000  5238.000000  0.858306   0.731025   \n",
       "mean                     593.454545  5913.545455  0.946752   0.579813   \n",
       "\n",
       "                          Recall        F1  \n",
       "related                 0.911777  0.878157  \n",
       "aid_related             0.619153  0.677959  \n",
       "medical_help            0.287908  0.384123  \n",
       "medical_products        0.317073  0.419355  \n",
       "search_and_rescue       0.171271  0.271930  \n",
       "security                0.042373  0.072464  \n",
       "military                0.330233  0.427711  \n",
       "water                   0.641148  0.674214  \n",
       "food                    0.674419  0.730370  \n",
       "shelter                 0.536332  0.634596  \n",
       "clothing                0.495050  0.581395  \n",
       "money                   0.264901  0.349345  \n",
       "missing_people          0.175676  0.270833  \n",
       "refugees                0.273973  0.371517  \n",
       "death                   0.476510  0.553606  \n",
       "other_aid               0.157159  0.239150  \n",
       "infrastructure_related  0.083721  0.134831  \n",
       "transport               0.250000  0.373134  \n",
       "buildings               0.396396  0.498113  \n",
       "electricity             0.240602  0.333333  \n",
       "tools                   0.025000  0.044444  \n",
       "hospitals               0.056338  0.086957  \n",
       "shops                   0.000000  0.000000  \n",
       "aid_centers             0.077922  0.122449  \n",
       "other_infrastructure    0.065972  0.104396  \n",
       "weather_related         0.675439  0.755596  \n",
       "floods                  0.571429  0.679912  \n",
       "storm                   0.638298  0.688438  \n",
       "fire                    0.242857  0.326923  \n",
       "earthquake              0.791803  0.827055  \n",
       "cold                    0.371212  0.492462  \n",
       "other_weather           0.180233  0.265525  \n",
       "direct_report           0.432624  0.543564  \n",
       "mean                    0.347721  0.418602  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metricss(y_test, y_pred_ada_len, y_test.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None, 'steps': [('features', FeatureUnion(n_jobs=None,\n",
       "                transformer_list=[('text_pipeline',\n",
       "                                   Pipeline(memory=None,\n",
       "                                            steps=[('vect',\n",
       "                                                    CountVectorizer(analyzer='word',\n",
       "                                                                    binary=False,\n",
       "                                                                    decode_error='strict',\n",
       "                                                                    dtype=<class 'numpy.int64'>,\n",
       "                                                                    encoding='utf-8',\n",
       "                                                                    input='content',\n",
       "                                                                    lowercase=True,\n",
       "                                                                    max_df=1.0,\n",
       "                                                                    max_features=None,\n",
       "                                                                    min_df=1,\n",
       "                                                                    ngram_range=(1,\n",
       "                                                                                 1),\n",
       "                                                                    preprocessor=None,\n",
       "                                                                    stop_words=None,\n",
       "                                                                    strip_accents=None,\n",
       "                                                                    tok...\n",
       "                                                                     use_idf=True))],\n",
       "                                            verbose=False)),\n",
       "                                  ('start_verb', StartVerbExtractor()),\n",
       "                                  ('length',\n",
       "                                   Pipeline(memory=None,\n",
       "                                            steps=[('text_length',\n",
       "                                                    FunctionTransformer(accept_sparse=False,\n",
       "                                                                        check_inverse=True,\n",
       "                                                                        func=<function get_text_len at 0x000001C48E780798>,\n",
       "                                                                        inv_kw_args=None,\n",
       "                                                                        inverse_func=None,\n",
       "                                                                        kw_args=None,\n",
       "                                                                        validate=False))],\n",
       "                                            verbose=False))],\n",
       "                transformer_weights=None, verbose=False)),\n",
       "  ('clf',\n",
       "   MultiOutputClassifier(estimator=AdaBoostClassifier(algorithm='SAMME.R',\n",
       "                                                      base_estimator=None,\n",
       "                                                      learning_rate=1.0,\n",
       "                                                      n_estimators=50,\n",
       "                                                      random_state=None),\n",
       "                         n_jobs=None))], 'verbose': False, 'features': FeatureUnion(n_jobs=None,\n",
       "              transformer_list=[('text_pipeline',\n",
       "                                 Pipeline(memory=None,\n",
       "                                          steps=[('vect',\n",
       "                                                  CountVectorizer(analyzer='word',\n",
       "                                                                  binary=False,\n",
       "                                                                  decode_error='strict',\n",
       "                                                                  dtype=<class 'numpy.int64'>,\n",
       "                                                                  encoding='utf-8',\n",
       "                                                                  input='content',\n",
       "                                                                  lowercase=True,\n",
       "                                                                  max_df=1.0,\n",
       "                                                                  max_features=None,\n",
       "                                                                  min_df=1,\n",
       "                                                                  ngram_range=(1,\n",
       "                                                                               1),\n",
       "                                                                  preprocessor=None,\n",
       "                                                                  stop_words=None,\n",
       "                                                                  strip_accents=None,\n",
       "                                                                  tok...\n",
       "                                                                   use_idf=True))],\n",
       "                                          verbose=False)),\n",
       "                                ('start_verb', StartVerbExtractor()),\n",
       "                                ('length',\n",
       "                                 Pipeline(memory=None,\n",
       "                                          steps=[('text_length',\n",
       "                                                  FunctionTransformer(accept_sparse=False,\n",
       "                                                                      check_inverse=True,\n",
       "                                                                      func=<function get_text_len at 0x000001C48E780798>,\n",
       "                                                                      inv_kw_args=None,\n",
       "                                                                      inverse_func=None,\n",
       "                                                                      kw_args=None,\n",
       "                                                                      validate=False))],\n",
       "                                          verbose=False))],\n",
       "              transformer_weights=None, verbose=False), 'clf': MultiOutputClassifier(estimator=AdaBoostClassifier(algorithm='SAMME.R',\n",
       "                                                    base_estimator=None,\n",
       "                                                    learning_rate=1.0,\n",
       "                                                    n_estimators=50,\n",
       "                                                    random_state=None),\n",
       "                       n_jobs=None), 'features__n_jobs': None, 'features__transformer_list': [('text_pipeline',\n",
       "   Pipeline(memory=None,\n",
       "            steps=[('vect',\n",
       "                    CountVectorizer(analyzer='word', binary=False,\n",
       "                                    decode_error='strict',\n",
       "                                    dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                    input='content', lowercase=True, max_df=1.0,\n",
       "                                    max_features=None, min_df=1,\n",
       "                                    ngram_range=(1, 1), preprocessor=None,\n",
       "                                    stop_words=None, strip_accents=None,\n",
       "                                    token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                    tokenizer=<function tokenize at 0x000001C48F6235E8>,\n",
       "                                    vocabulary=None)),\n",
       "                   ('tfidf',\n",
       "                    TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                     sublinear_tf=False, use_idf=True))],\n",
       "            verbose=False)),\n",
       "  ('start_verb', StartVerbExtractor()),\n",
       "  ('length', Pipeline(memory=None,\n",
       "            steps=[('text_length',\n",
       "                    FunctionTransformer(accept_sparse=False, check_inverse=True,\n",
       "                                        func=<function get_text_len at 0x000001C48E780798>,\n",
       "                                        inv_kw_args=None, inverse_func=None,\n",
       "                                        kw_args=None, validate=False))],\n",
       "            verbose=False))], 'features__transformer_weights': None, 'features__verbose': False, 'features__text_pipeline': Pipeline(memory=None,\n",
       "          steps=[('vect',\n",
       "                  CountVectorizer(analyzer='word', binary=False,\n",
       "                                  decode_error='strict',\n",
       "                                  dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                  input='content', lowercase=True, max_df=1.0,\n",
       "                                  max_features=None, min_df=1,\n",
       "                                  ngram_range=(1, 1), preprocessor=None,\n",
       "                                  stop_words=None, strip_accents=None,\n",
       "                                  token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                  tokenizer=<function tokenize at 0x000001C48F6235E8>,\n",
       "                                  vocabulary=None)),\n",
       "                 ('tfidf',\n",
       "                  TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                   sublinear_tf=False, use_idf=True))],\n",
       "          verbose=False), 'features__start_verb': StartVerbExtractor(), 'features__length': Pipeline(memory=None,\n",
       "          steps=[('text_length',\n",
       "                  FunctionTransformer(accept_sparse=False, check_inverse=True,\n",
       "                                      func=<function get_text_len at 0x000001C48E780798>,\n",
       "                                      inv_kw_args=None, inverse_func=None,\n",
       "                                      kw_args=None, validate=False))],\n",
       "          verbose=False), 'features__text_pipeline__memory': None, 'features__text_pipeline__steps': [('vect',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function tokenize at 0x000001C48F6235E8>,\n",
       "                   vocabulary=None)),\n",
       "  ('tfidf',\n",
       "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True))], 'features__text_pipeline__verbose': False, 'features__text_pipeline__vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function tokenize at 0x000001C48F6235E8>,\n",
       "                 vocabulary=None), 'features__text_pipeline__tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True), 'features__text_pipeline__vect__analyzer': 'word', 'features__text_pipeline__vect__binary': False, 'features__text_pipeline__vect__decode_error': 'strict', 'features__text_pipeline__vect__dtype': numpy.int64, 'features__text_pipeline__vect__encoding': 'utf-8', 'features__text_pipeline__vect__input': 'content', 'features__text_pipeline__vect__lowercase': True, 'features__text_pipeline__vect__max_df': 1.0, 'features__text_pipeline__vect__max_features': None, 'features__text_pipeline__vect__min_df': 1, 'features__text_pipeline__vect__ngram_range': (1,\n",
       "  1), 'features__text_pipeline__vect__preprocessor': None, 'features__text_pipeline__vect__stop_words': None, 'features__text_pipeline__vect__strip_accents': None, 'features__text_pipeline__vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'features__text_pipeline__vect__tokenizer': <function __main__.tokenize(text)>, 'features__text_pipeline__vect__vocabulary': None, 'features__text_pipeline__tfidf__norm': 'l2', 'features__text_pipeline__tfidf__smooth_idf': True, 'features__text_pipeline__tfidf__sublinear_tf': False, 'features__text_pipeline__tfidf__use_idf': True, 'features__length__memory': None, 'features__length__steps': [('text_length',\n",
       "   FunctionTransformer(accept_sparse=False, check_inverse=True,\n",
       "                       func=<function get_text_len at 0x000001C48E780798>,\n",
       "                       inv_kw_args=None, inverse_func=None, kw_args=None,\n",
       "                       validate=False))], 'features__length__verbose': False, 'features__length__text_length': FunctionTransformer(accept_sparse=False, check_inverse=True,\n",
       "                     func=<function get_text_len at 0x000001C48E780798>,\n",
       "                     inv_kw_args=None, inverse_func=None, kw_args=None,\n",
       "                     validate=False), 'features__length__text_length__accept_sparse': False, 'features__length__text_length__check_inverse': True, 'features__length__text_length__func': <function __main__.get_text_len(data)>, 'features__length__text_length__inv_kw_args': None, 'features__length__text_length__inverse_func': None, 'features__length__text_length__kw_args': None, 'features__length__text_length__validate': False, 'clf__estimator__algorithm': 'SAMME.R', 'clf__estimator__base_estimator': None, 'clf__estimator__learning_rate': 1.0, 'clf__estimator__n_estimators': 50, 'clf__estimator__random_state': None, 'clf__estimator': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None), 'clf__n_jobs': None}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ada_len.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model with LogisticRegression\n",
    "\n",
    "Training showed that LogisticRegression is worst than Adaboost and RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Logistic_pipeline():\n",
    "    pipeline = Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "\n",
    "            ('text_pipeline', Pipeline([\n",
    "                ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "                ('tfidf', TfidfTransformer())\n",
    "            ])),\n",
    "\n",
    "            ('start_verb', StartVerbExtractor())\n",
    "        ])),\n",
    "\n",
    "        ('clf', MultiOutputClassifier(LogisticRegression()))\n",
    "    ])\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_log = Logistic_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.seed(42)\n",
    "#model_log.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred_log = model_log.predict(X_test)\n",
    "#y_pred_log = pd.DataFrame(y_pred_log, columns=y_test.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metricss(y_test, y_pred_log, y_test.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_msg = ['There is a fire in building nearby and there are people in it.']\n",
    "#test = model_log.predict(test_msg)\n",
    "#print(y_train.columns.values[(test.flatten()==1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model with randomforest and start verb Extractor\n",
    "\n",
    "The recall is worst than adaboost, and since it is a disaster response, it is better to have a higher recall score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_pipeline():\n",
    "    pipeline = Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "\n",
    "            ('text_pipeline', Pipeline([\n",
    "                ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "                ('tfidf', TfidfTransformer())\n",
    "            ])),\n",
    "\n",
    "            ('start_verb', StartVerbExtractor())\n",
    "        ])),\n",
    "\n",
    "        ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "    ])\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_stem = simple_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.seed(42)\n",
    "#model_stem.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred_stem = model_stem.predict(X_test)\n",
    "#y_pred_stem = pd.DataFrame(y_pred_stem, columns=y_test.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metricss(y_test, y_pred_stem, y_test.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model with randomforest and start verb Extractor AND text_len\n",
    "\n",
    "Training showed that LogisticRegression is worst than just with the startverbExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_pipeline2():\n",
    "    pipeline = Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "\n",
    "            ('text_pipeline', Pipeline([\n",
    "                ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "                ('tfidf', TfidfTransformer())\n",
    "            ])),\n",
    "\n",
    "            ('start_verb', StartVerbExtractor()),\n",
    "            ('length', Pipeline([('text_length', FunctionTransformer(get_text_len, validate=False))]))\n",
    "        ])),\n",
    "\n",
    "        ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "    ])\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_stem_len = simple_pipeline2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.seed(42)\n",
    "#model_stem_len.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred_stem_len = model_stem_len.predict(X_test)\n",
    "#y_pred_stem_len = pd.DataFrame(y_pred_stem_len, columns=y_test.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metricss(y_test, y_pred_stem_len, y_test.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model with randomforest and text_len\n",
    "\n",
    "Training showed that LogisticRegression is worst than just with the startverbExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_pipeline3():\n",
    "    pipeline = Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "\n",
    "            ('text_pipeline', Pipeline([\n",
    "                ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "                ('tfidf', TfidfTransformer())\n",
    "            ])),\n",
    "\n",
    "            ('length', Pipeline([('text_length', FunctionTransformer(get_text_len, validate=False))]))\n",
    "        ])),\n",
    "\n",
    "        ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "    ])\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_len = simple_pipeline3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.seed(42)\n",
    "#model_len.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred_len = model_len.predict(X_test)\n",
    "#y_pred_len = pd.DataFrame(y_pred_len, columns=y_test.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metricss(y_test, y_pred_len, y_test.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the model\n",
    "\n",
    "Since in the case of a disaster is worst to have a false negative than a false positive, I will make a scorer that will try to improve more Recall than Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_metric(y_true, y_pred):\n",
    "    \"\"\"Calculate mean Recall \n",
    "    \n",
    "    Args:\n",
    "    y_true: array. Array containing actual labels.\n",
    "    y_pred: array. Array containing predicted labels.\n",
    "        \n",
    "    Returns:\n",
    "    score: float. Median F1 score for all of the output classifiers\n",
    "    \"\"\"\n",
    "    score_list = []\n",
    "    for i in range(np.shape(y_pred)[1]):\n",
    "        score_i = (recall_score(np.array(y_true)[:, i], y_pred[:, i]) * 3 + \n",
    "                   precision_score(np.array(y_true)[:, i], y_pred[:, i])) / 4\n",
    "        score_list.append(score_i)\n",
    "        \n",
    "    score = np.mean(score_list)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'features__text_pipeline__vect__ngram_range':[(1,2),(2,2)],\n",
    "            'clf__estimator__n_estimators':[50, 100, 300]\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_adaboost_len = Adaboost_len_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done   3 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=5)]: Done   8 tasks      | elapsed:  8.9min\n",
      "[Parallel(n_jobs=5)]: Done  15 tasks      | elapsed: 18.4min\n",
      "[Parallel(n_jobs=5)]: Done  25 out of  30 | elapsed: 49.5min remaining:  9.9min\n",
      "[Parallel(n_jobs=5)]: Done  30 out of  30 | elapsed: 63.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('features',\n",
       "                                        FeatureUnion(n_jobs=None,\n",
       "                                                     transformer_list=[('text_pipeline',\n",
       "                                                                        Pipeline(memory=None,\n",
       "                                                                                 steps=[('vect',\n",
       "                                                                                         CountVectorizer(analyzer='word',\n",
       "                                                                                                         binary=False,\n",
       "                                                                                                         decode_error='strict',\n",
       "                                                                                                         dtype=<class 'numpy.int64'>,\n",
       "                                                                                                         encoding='utf-8',\n",
       "                                                                                                         input='content',\n",
       "                                                                                                         lowercase=True,\n",
       "                                                                                                         max_df=1.0,\n",
       "                                                                                                         max_features=N...\n",
       "                                                                                           base_estimator=None,\n",
       "                                                                                           learning_rate=1.0,\n",
       "                                                                                           n_estimators=50,\n",
       "                                                                                           random_state=None),\n",
       "                                                              n_jobs=None))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=5,\n",
       "             param_grid={'clf__estimator__n_estimators': [50, 100, 300],\n",
       "                         'features__text_pipeline__vect__ngram_range': [(1, 2),\n",
       "                                                                        (2,\n",
       "                                                                         2)]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=make_scorer(recall_metric), verbose=10)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scorer = make_scorer(recall_metric)\n",
    "np.random.seed(42)\n",
    "cv = GridSearchCV(pipeline_adaboost_len, param_grid=parameters, scoring=scorer, n_jobs=5, verbose=10)\n",
    "cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cv_pred = cv.predict(X_test)\n",
    "y_cv_pred = pd.DataFrame(y_cv_pred, columns=y_test.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># 1s</th>\n",
       "      <th># 0s</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>related</th>\n",
       "      <td>4976.000000</td>\n",
       "      <td>1531.000000</td>\n",
       "      <td>0.818196</td>\n",
       "      <td>0.872960</td>\n",
       "      <td>0.892082</td>\n",
       "      <td>0.882417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aid_related</th>\n",
       "      <td>2715.000000</td>\n",
       "      <td>3792.000000</td>\n",
       "      <td>0.759336</td>\n",
       "      <td>0.725383</td>\n",
       "      <td>0.681031</td>\n",
       "      <td>0.702508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medical_help</th>\n",
       "      <td>521.000000</td>\n",
       "      <td>5986.000000</td>\n",
       "      <td>0.918396</td>\n",
       "      <td>0.487310</td>\n",
       "      <td>0.368522</td>\n",
       "      <td>0.419672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medical_products</th>\n",
       "      <td>328.000000</td>\n",
       "      <td>6179.000000</td>\n",
       "      <td>0.946980</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.414634</td>\n",
       "      <td>0.440843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>search_and_rescue</th>\n",
       "      <td>181.000000</td>\n",
       "      <td>6326.000000</td>\n",
       "      <td>0.965268</td>\n",
       "      <td>0.335766</td>\n",
       "      <td>0.254144</td>\n",
       "      <td>0.289308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>security</th>\n",
       "      <td>118.000000</td>\n",
       "      <td>6389.000000</td>\n",
       "      <td>0.970186</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.101695</td>\n",
       "      <td>0.110092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>military</th>\n",
       "      <td>215.000000</td>\n",
       "      <td>6292.000000</td>\n",
       "      <td>0.963578</td>\n",
       "      <td>0.447115</td>\n",
       "      <td>0.432558</td>\n",
       "      <td>0.439716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>water</th>\n",
       "      <td>418.000000</td>\n",
       "      <td>6089.000000</td>\n",
       "      <td>0.955894</td>\n",
       "      <td>0.646532</td>\n",
       "      <td>0.691388</td>\n",
       "      <td>0.668208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>food</th>\n",
       "      <td>731.000000</td>\n",
       "      <td>5776.000000</td>\n",
       "      <td>0.939450</td>\n",
       "      <td>0.731139</td>\n",
       "      <td>0.729138</td>\n",
       "      <td>0.730137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shelter</th>\n",
       "      <td>578.000000</td>\n",
       "      <td>5929.000000</td>\n",
       "      <td>0.933917</td>\n",
       "      <td>0.635531</td>\n",
       "      <td>0.600346</td>\n",
       "      <td>0.617438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clothing</th>\n",
       "      <td>101.000000</td>\n",
       "      <td>6406.000000</td>\n",
       "      <td>0.985093</td>\n",
       "      <td>0.520408</td>\n",
       "      <td>0.504950</td>\n",
       "      <td>0.512563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>money</th>\n",
       "      <td>151.000000</td>\n",
       "      <td>6356.000000</td>\n",
       "      <td>0.969879</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.417219</td>\n",
       "      <td>0.391304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>missing_people</th>\n",
       "      <td>74.000000</td>\n",
       "      <td>6433.000000</td>\n",
       "      <td>0.983402</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.229730</td>\n",
       "      <td>0.239437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>refugees</th>\n",
       "      <td>219.000000</td>\n",
       "      <td>6288.000000</td>\n",
       "      <td>0.960350</td>\n",
       "      <td>0.402985</td>\n",
       "      <td>0.369863</td>\n",
       "      <td>0.385714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>death</th>\n",
       "      <td>298.000000</td>\n",
       "      <td>6209.000000</td>\n",
       "      <td>0.957584</td>\n",
       "      <td>0.543651</td>\n",
       "      <td>0.459732</td>\n",
       "      <td>0.498182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other_aid</th>\n",
       "      <td>859.000000</td>\n",
       "      <td>5648.000000</td>\n",
       "      <td>0.846166</td>\n",
       "      <td>0.370909</td>\n",
       "      <td>0.237485</td>\n",
       "      <td>0.289567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>infrastructure_related</th>\n",
       "      <td>430.000000</td>\n",
       "      <td>6077.000000</td>\n",
       "      <td>0.912095</td>\n",
       "      <td>0.261745</td>\n",
       "      <td>0.181395</td>\n",
       "      <td>0.214286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transport</th>\n",
       "      <td>300.000000</td>\n",
       "      <td>6207.000000</td>\n",
       "      <td>0.948824</td>\n",
       "      <td>0.424658</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>0.358382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>buildings</th>\n",
       "      <td>333.000000</td>\n",
       "      <td>6174.000000</td>\n",
       "      <td>0.948363</td>\n",
       "      <td>0.494881</td>\n",
       "      <td>0.435435</td>\n",
       "      <td>0.463259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>electricity</th>\n",
       "      <td>133.000000</td>\n",
       "      <td>6374.000000</td>\n",
       "      <td>0.977716</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.406015</td>\n",
       "      <td>0.426877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tools</th>\n",
       "      <td>40.000000</td>\n",
       "      <td>6467.000000</td>\n",
       "      <td>0.989857</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.057143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hospitals</th>\n",
       "      <td>71.000000</td>\n",
       "      <td>6436.000000</td>\n",
       "      <td>0.984017</td>\n",
       "      <td>0.188679</td>\n",
       "      <td>0.140845</td>\n",
       "      <td>0.161290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shops</th>\n",
       "      <td>30.000000</td>\n",
       "      <td>6477.000000</td>\n",
       "      <td>0.993545</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aid_centers</th>\n",
       "      <td>77.000000</td>\n",
       "      <td>6430.000000</td>\n",
       "      <td>0.979714</td>\n",
       "      <td>0.160494</td>\n",
       "      <td>0.168831</td>\n",
       "      <td>0.164557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other_infrastructure</th>\n",
       "      <td>288.000000</td>\n",
       "      <td>6219.000000</td>\n",
       "      <td>0.931612</td>\n",
       "      <td>0.192157</td>\n",
       "      <td>0.170139</td>\n",
       "      <td>0.180479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weather_related</th>\n",
       "      <td>1824.000000</td>\n",
       "      <td>4683.000000</td>\n",
       "      <td>0.880744</td>\n",
       "      <td>0.817961</td>\n",
       "      <td>0.739035</td>\n",
       "      <td>0.776498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>floods</th>\n",
       "      <td>539.000000</td>\n",
       "      <td>5968.000000</td>\n",
       "      <td>0.939296</td>\n",
       "      <td>0.644000</td>\n",
       "      <td>0.597403</td>\n",
       "      <td>0.619827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>storm</th>\n",
       "      <td>611.000000</td>\n",
       "      <td>5896.000000</td>\n",
       "      <td>0.935147</td>\n",
       "      <td>0.654160</td>\n",
       "      <td>0.656301</td>\n",
       "      <td>0.655229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fire</th>\n",
       "      <td>70.000000</td>\n",
       "      <td>6437.000000</td>\n",
       "      <td>0.986322</td>\n",
       "      <td>0.349206</td>\n",
       "      <td>0.314286</td>\n",
       "      <td>0.330827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>earthquake</th>\n",
       "      <td>610.000000</td>\n",
       "      <td>5897.000000</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.811130</td>\n",
       "      <td>0.788525</td>\n",
       "      <td>0.799667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cold</th>\n",
       "      <td>132.000000</td>\n",
       "      <td>6375.000000</td>\n",
       "      <td>0.978792</td>\n",
       "      <td>0.475410</td>\n",
       "      <td>0.439394</td>\n",
       "      <td>0.456693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other_weather</th>\n",
       "      <td>344.000000</td>\n",
       "      <td>6163.000000</td>\n",
       "      <td>0.925158</td>\n",
       "      <td>0.254296</td>\n",
       "      <td>0.215116</td>\n",
       "      <td>0.233071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>direct_report</th>\n",
       "      <td>1269.000000</td>\n",
       "      <td>5238.000000</td>\n",
       "      <td>0.849700</td>\n",
       "      <td>0.634101</td>\n",
       "      <td>0.542159</td>\n",
       "      <td>0.584537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>593.454545</td>\n",
       "      <td>5913.545455</td>\n",
       "      <td>0.939319</td>\n",
       "      <td>0.448735</td>\n",
       "      <td>0.410285</td>\n",
       "      <td>0.427264</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               # 1s         # 0s  Accuracy  Precision  \\\n",
       "related                 4976.000000  1531.000000  0.818196   0.872960   \n",
       "aid_related             2715.000000  3792.000000  0.759336   0.725383   \n",
       "medical_help             521.000000  5986.000000  0.918396   0.487310   \n",
       "medical_products         328.000000  6179.000000  0.946980   0.470588   \n",
       "search_and_rescue        181.000000  6326.000000  0.965268   0.335766   \n",
       "security                 118.000000  6389.000000  0.970186   0.120000   \n",
       "military                 215.000000  6292.000000  0.963578   0.447115   \n",
       "water                    418.000000  6089.000000  0.955894   0.646532   \n",
       "food                     731.000000  5776.000000  0.939450   0.731139   \n",
       "shelter                  578.000000  5929.000000  0.933917   0.635531   \n",
       "clothing                 101.000000  6406.000000  0.985093   0.520408   \n",
       "money                    151.000000  6356.000000  0.969879   0.368421   \n",
       "missing_people            74.000000  6433.000000  0.983402   0.250000   \n",
       "refugees                 219.000000  6288.000000  0.960350   0.402985   \n",
       "death                    298.000000  6209.000000  0.957584   0.543651   \n",
       "other_aid                859.000000  5648.000000  0.846166   0.370909   \n",
       "infrastructure_related   430.000000  6077.000000  0.912095   0.261745   \n",
       "transport                300.000000  6207.000000  0.948824   0.424658   \n",
       "buildings                333.000000  6174.000000  0.948363   0.494881   \n",
       "electricity              133.000000  6374.000000  0.977716   0.450000   \n",
       "tools                     40.000000  6467.000000  0.989857   0.066667   \n",
       "hospitals                 71.000000  6436.000000  0.984017   0.188679   \n",
       "shops                     30.000000  6477.000000  0.993545   0.000000   \n",
       "aid_centers               77.000000  6430.000000  0.979714   0.160494   \n",
       "other_infrastructure     288.000000  6219.000000  0.931612   0.192157   \n",
       "weather_related         1824.000000  4683.000000  0.880744   0.817961   \n",
       "floods                   539.000000  5968.000000  0.939296   0.644000   \n",
       "storm                    611.000000  5896.000000  0.935147   0.654160   \n",
       "fire                      70.000000  6437.000000  0.986322   0.349206   \n",
       "earthquake               610.000000  5897.000000  0.962963   0.811130   \n",
       "cold                     132.000000  6375.000000  0.978792   0.475410   \n",
       "other_weather            344.000000  6163.000000  0.925158   0.254296   \n",
       "direct_report           1269.000000  5238.000000  0.849700   0.634101   \n",
       "mean                     593.454545  5913.545455  0.939319   0.448735   \n",
       "\n",
       "                          Recall        F1  \n",
       "related                 0.892082  0.882417  \n",
       "aid_related             0.681031  0.702508  \n",
       "medical_help            0.368522  0.419672  \n",
       "medical_products        0.414634  0.440843  \n",
       "search_and_rescue       0.254144  0.289308  \n",
       "security                0.101695  0.110092  \n",
       "military                0.432558  0.439716  \n",
       "water                   0.691388  0.668208  \n",
       "food                    0.729138  0.730137  \n",
       "shelter                 0.600346  0.617438  \n",
       "clothing                0.504950  0.512563  \n",
       "money                   0.417219  0.391304  \n",
       "missing_people          0.229730  0.239437  \n",
       "refugees                0.369863  0.385714  \n",
       "death                   0.459732  0.498182  \n",
       "other_aid               0.237485  0.289567  \n",
       "infrastructure_related  0.181395  0.214286  \n",
       "transport               0.310000  0.358382  \n",
       "buildings               0.435435  0.463259  \n",
       "electricity             0.406015  0.426877  \n",
       "tools                   0.050000  0.057143  \n",
       "hospitals               0.140845  0.161290  \n",
       "shops                   0.000000  0.000000  \n",
       "aid_centers             0.168831  0.164557  \n",
       "other_infrastructure    0.170139  0.180479  \n",
       "weather_related         0.739035  0.776498  \n",
       "floods                  0.597403  0.619827  \n",
       "storm                   0.656301  0.655229  \n",
       "fire                    0.314286  0.330827  \n",
       "earthquake              0.788525  0.799667  \n",
       "cold                    0.439394  0.456693  \n",
       "other_weather           0.215116  0.233071  \n",
       "direct_report           0.542159  0.584537  \n",
       "mean                    0.410285  0.427264  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metricss(y_test, y_cv_pred, y_test.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 286.11497765,  214.34299169,  500.95988688,  355.87668743,\n",
       "        1362.29578128,  907.69777088]),\n",
       " 'std_fit_time': array([ 18.27465811,  21.81364108,  35.58426586,  37.43845512,\n",
       "        108.89083467,  95.4036249 ]),\n",
       " 'mean_score_time': array([22.25593967, 20.91288867, 25.14417477, 23.73936653, 40.48160877,\n",
       "        28.26198945]),\n",
       " 'std_score_time': array([3.8203817 , 1.73477563, 3.53129051, 0.8291392 , 4.97029861,\n",
       "        5.44547765]),\n",
       " 'param_clf__estimator__n_estimators': masked_array(data=[50, 50, 100, 100, 300, 300],\n",
       "              mask=[False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_features__text_pipeline__vect__ngram_range': masked_array(data=[(1, 2), (2, 2), (1, 2), (2, 2), (1, 2), (2, 2)],\n",
       "              mask=[False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'clf__estimator__n_estimators': 50,\n",
       "   'features__text_pipeline__vect__ngram_range': (1, 2)},\n",
       "  {'clf__estimator__n_estimators': 50,\n",
       "   'features__text_pipeline__vect__ngram_range': (2, 2)},\n",
       "  {'clf__estimator__n_estimators': 100,\n",
       "   'features__text_pipeline__vect__ngram_range': (1, 2)},\n",
       "  {'clf__estimator__n_estimators': 100,\n",
       "   'features__text_pipeline__vect__ngram_range': (2, 2)},\n",
       "  {'clf__estimator__n_estimators': 300,\n",
       "   'features__text_pipeline__vect__ngram_range': (1, 2)},\n",
       "  {'clf__estimator__n_estimators': 300,\n",
       "   'features__text_pipeline__vect__ngram_range': (2, 2)}],\n",
       " 'split0_test_score': array([0.30462081, 0.21712775, 0.30814892, 0.21477764, 0.30622741,\n",
       "        0.21189924]),\n",
       " 'split1_test_score': array([0.31936657, 0.20172357, 0.29967768, 0.19987529, 0.31004715,\n",
       "        0.19623329]),\n",
       " 'split2_test_score': array([0.36236186, 0.22905458, 0.37413861, 0.23391672, 0.38758083,\n",
       "        0.23280046]),\n",
       " 'split3_test_score': array([0.38355954, 0.24811416, 0.39381872, 0.25449743, 0.38917008,\n",
       "        0.25506101]),\n",
       " 'split4_test_score': array([0.39745717, 0.24872879, 0.39593244, 0.2502149 , 0.38285252,\n",
       "        0.24982023]),\n",
       " 'mean_test_score': array([0.35347319, 0.22894977, 0.35434327, 0.2306564 , 0.3551756 ,\n",
       "        0.22916285]),\n",
       " 'std_test_score': array([0.03596833, 0.01810812, 0.04195729, 0.02079024, 0.03848178,\n",
       "        0.02232963]),\n",
       " 'rank_test_score': array([3, 6, 2, 4, 1, 5])}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__estimator__n_estimators': 300,\n",
       " 'features__text_pipeline__vect__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test message below, is an arbitrary test, just to check if it gets the mean point here: some place is on Fire, therefore the firefighters should be called. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['related' 'fire']\n"
     ]
    }
   ],
   "source": [
    "test_msg = ['There is a fire in building nearby and there are people in it.']\n",
    "test = cv.predict(test_msg)\n",
    "print(y_train.columns.values[(test.flatten()==1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that the 'fire' feature was predicted, so with a simple script the firefighters could be called. It worked!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best resul is for:\n",
    "\n",
    "- AdaboosClassifier\n",
    "\n",
    "- Using StartVerbExtractor\n",
    "\n",
    "- Using text_len\n",
    "\n",
    "- with vect_ngram_range (1,2)\n",
    "\n",
    "- with n_estimators 300."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle best model\n",
    "pickle.dump(cv, open('disaster_model_hnbez.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3-TF2.0]",
   "language": "python",
   "name": "conda-env-py3-TF2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
