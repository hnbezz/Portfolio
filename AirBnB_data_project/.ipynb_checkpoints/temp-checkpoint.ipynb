{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error, accuracy_score\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.svm import SVR\n",
    "import seaborn as sns\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Seatle's data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_calendar_data = pd.read_csv('Seatle_calendar.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_l_data = pd.read_csv('Seatle_listings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_reviews_data = pd.read_csv('Seatle_reviews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the listings data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing url features\n",
    "col_url = []\n",
    "for col in S_l_data.columns.tolist():\n",
    "    if '_url' in col:\n",
    "        col_url.append(col)\n",
    "\n",
    "S_l_dropped2 = S_l_data.drop(col_url, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I am removing some features that obviously lead to nothing, such as #id and 'name'.**\n",
    "\n",
    "**I have to explain some of the removals, such as 'street', 'zipcode', 'host_listings_count', 'latitude', 'neighbourhood_cleansed', neighbourhood_group_cleansed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_l_dropped3 = S_l_dropped2.drop(['#id','name','host_id','host_name','host_id','smart_location','state', \n",
    "                                  'host_listings_count','street', 'host_location', \n",
    "                                  'host_neighbourhood', 'neighbourhood_group_cleansed','neighbourhood_cleansed','city'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I also need to transform the dates that are in string format to date format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = re.compile('.{4}-.{2}-.{2}')\n",
    "\n",
    "for col in S_l_dropped3.columns.tolist():\n",
    "    if S_l_dropped3[col].dtypes == 'O':\n",
    "        if r.match(S_l_dropped3[col].any()):\n",
    "            S_l_dropped3[col] = pd.to_datetime(S_l_dropped3[col], errors='ignore')\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I will also drop features that I cannot really evaluate considering the knowledge I have. However I will create a variable to check if the feature is offered or not. For example, there are some listings that do not inform the \"Neighborhood overview\", So i will drop the overview and create a boolean variable called \"neighborhood_overview_given\" to indicate if it was informed. This I do manually, because I can't find a way to do it systematically for the different features.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_drop2 = ['summary', 'space', 'neighborhood_overview', 'description', 'transit', 'notes', 'host_about']\n",
    "\n",
    "for col in col_to_drop2:\n",
    "    S_l_dropped3['isthere_'+col] = ''\n",
    "    S_l_dropped3['isthere_'+col] = pd.Series(S_l_dropped3[col].isna().to_numpy(dtype=int))\n",
    "\n",
    "\n",
    "S_l_dropped4 = S_l_dropped3.drop(col_to_drop2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing features which do no vary.\n",
    "col_to_drop = []\n",
    "for col in S_l_dropped4.columns.tolist():\n",
    "    if len(S_l_dropped4[col].unique())==1:\n",
    "        col_to_drop.append(col)\n",
    "\n",
    "S_l_dropped4 = S_l_dropped4.drop(col_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I'll remove the outliers here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_vars1 =  S_l_dropped4.select_dtypes(include=['float', 'int']).columns\n",
    "for col in num_vars1:\n",
    "    q = S_l_dropped4[col].quantile(0.99)\n",
    "    S_l_dropped5 = S_l_dropped4[S_l_dropped4[col]<q]\n",
    "S_l_dropped5 = S_l_dropped5.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now I transform the amenities and host verifications into several columns with 1 for present, an 0 for not**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(S_l_dropped5.shape[0]):\n",
    "    if '{'  in S_l_dropped5['amenities'][i]:\n",
    "        S_l_dropped5.loc[i,'amenities'] = S_l_dropped5['amenities'][i].replace('{','')\n",
    "    if '}' in S_l_dropped5['amenities'][i]:\n",
    "        S_l_dropped5.loc[i,'amenities'] = S_l_dropped5['amenities'][i].replace('}','')\n",
    "    if '\"' in S_l_dropped5['amenities'][i]:\n",
    "        S_l_dropped5.loc[i,'amenities'] = S_l_dropped5['amenities'][i].replace('\"','')\n",
    "\n",
    "all_am=np.array([])\n",
    "for i in range(S_l_dropped5.shape[0]):\n",
    "    all_am = np.append(all_am, S_l_dropped5['amenities'][i].split(','))\n",
    "\n",
    "all_am = np.unique(all_am)\n",
    "all_am = all_am[1:] # This is because it created a '' (empty) case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for val in all_am:\n",
    "    S_l_dropped5['amen_'+val] = ''\n",
    "\n",
    "S_l_amen = S_l_dropped5.copy()\n",
    "#for i in range(S_l_dropped5.shape[0]):\n",
    "#    for val in all_am:\n",
    "#        if val in S_l_dropped5['amenities'][i]:\n",
    "#            S_l_amen.loc[i,'amen_'+val] = 1        \n",
    "#        else:\n",
    "#            S_l_amen.loc[i,'amen_'+val] = 0\n",
    "\n",
    "for val in all_am:\n",
    "    S_l_amen['amen_'+val] = pd.Series(S_l_amen['amenities'].str.contains(val).to_numpy(dtype=int))\n",
    "    \n",
    "for i in range(S_l_amen.shape[0]):\n",
    "    if '['  in S_l_amen['host_verifications'][i]:\n",
    "        S_l_amen.loc[i,'host_verifications'] = S_l_amen['host_verifications'][i].replace('[','')\n",
    "    if ']' in S_l_dropped5['host_verifications'][i]:\n",
    "        S_l_amen.loc[i,'host_verifications'] = S_l_amen['host_verifications'][i].replace(']','')\n",
    "    if '\"' in S_l_dropped5['host_verifications'][i]:\n",
    "        S_l_amen.loc[i,'host_verifications'] = S_l_amen['host_verifications'][i].replace('\"','')\n",
    "    if \"'\" in S_l_dropped5['host_verifications'][i]:\n",
    "        S_l_amen.loc[i,'host_verifications'] = S_l_amen['host_verifications'][i].replace(\"'\",'')\n",
    "    if \" \" in S_l_dropped5['host_verifications'][i]:\n",
    "        S_l_amen.loc[i,'host_verifications'] = S_l_amen['host_verifications'][i].replace(\" \",'')\n",
    "\n",
    "all_ver=np.array([])\n",
    "for i in range(S_l_amen.shape[0]):\n",
    "    all_ver = np.append(all_ver, S_l_amen['host_verifications'][i].split(','))\n",
    "    \n",
    "all_ver = np.unique(all_ver)\n",
    "all_ver = all_ver[1:] # This is because it created a '' (empty) case, also because there is a 'None' verification\n",
    "\n",
    "for val in all_ver:\n",
    "    S_l_amen['verif_' + val] = ''\n",
    "\n",
    "S_l_amenver = S_l_amen.copy()\n",
    "\n",
    "for val in all_ver:\n",
    "    S_l_amenver['verif_'+val] = pd.Series(S_l_amenver['host_verifications'].str.contains(val).to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_l_amenver = S_l_amenver.drop(['host_verifications','amenities','isthere_host_about'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I also drop any amenities or host verification, that less than 1% of the listings have (a.k.a. outliers)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_l_amenver.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_drop3=[]\n",
    "for val1 in all_am:\n",
    "    if S_l_amenver['amen_' + val1].sum() < int(0.01*S_l_amenver.shape[0]):\n",
    "        col_to_drop3.append('amen_'+val1)\n",
    "S_l_amenver = S_l_amenver.drop(col_to_drop3,axis=1)\n",
    "\n",
    "col_to_drop4=[]\n",
    "\n",
    "for val1 in all_ver:\n",
    "    if S_l_amenver['verif_' + val1].sum() < int(0.01*S_l_amenver.shape[0]):\n",
    "        col_to_drop4.append('verif_'+val1)\n",
    "col_to_drop4\n",
    "S_l_amenver = S_l_amenver.drop(col_to_drop4,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I make the features with 't' and 'f' values into 1 and 0's.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_tf = []\n",
    "for col in S_l_amenver.columns.tolist():\n",
    "    if ('f' or 't') in S_l_amenver[col].unique():\n",
    "        col_tf.append(col)    \n",
    "\n",
    "for col in col_tf:\n",
    "    S_l_amenver[col] = S_l_amenver[col].replace(['t','f'],[int(1),int(0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here I handle the dates, making them into a difference between dates (ends up as an integer)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_l_amenver['host_since'] = (S_l_amenver['host_since']-S_l_amenver['host_since'].min()).dt.days\n",
    "S_l_amenver['first_review'] = (S_l_amenver['first_review']-S_l_amenver['first_review'].min()).dt.days\n",
    "S_l_amenver['last_review'] = (S_l_amenver['last_review']-S_l_amenver['last_review'].min()).dt.days\n",
    "\n",
    "for i in range(S_l_amenver.shape[0]):\n",
    "    if 'weeks ago' in S_l_amenver['calendar_updated'][i]:\n",
    "        S_l_amenver.loc[i,'calendar_updated'] = float(S_l_amenver['calendar_updated'][i].replace(' weeks ago', '')) * 7\n",
    "    elif 'a week ago' in S_l_amenver['calendar_updated'][i]:\n",
    "        S_l_amenver.loc[i,'calendar_updated'] = float(S_l_amenver['calendar_updated'][i].replace('a week ago', '1')) * 7\n",
    "    elif '1 week ago' in S_l_amenver['calendar_updated'][i]:\n",
    "        S_l_amenver.loc[i,'calendar_updated'] = float(S_l_amenver['calendar_updated'][i].replace('1 week ago', '1')) * 7\n",
    "    elif 'yesterday' in S_l_amenver['calendar_updated'][i]:\n",
    "        S_l_amenver.loc[i,'calendar_updated'] = float(S_l_amenver['calendar_updated'][i].replace('yesterday', '1')) * 1\n",
    "    elif 'days ago' in S_l_amenver['calendar_updated'][i]:\n",
    "        S_l_amenver.loc[i,'calendar_updated'] = float(S_l_amenver['calendar_updated'][i].replace(' days ago', '1')) * 1\n",
    "    elif 'today' in S_l_amenver['calendar_updated'][i]:\n",
    "        S_l_amenver.loc[i,'calendar_updated'] = float(S_l_amenver['calendar_updated'][i].replace('today', '1')) * 1\n",
    "    elif 'never' in S_l_amenver['calendar_updated'][i]:\n",
    "        S_l_amenver.loc[i,'calendar_updated'] = float(S_l_amenver['calendar_updated'][i].replace('never', '1')) * 2000\n",
    "    elif 'months ago' in S_l_amenver['calendar_updated'][i]:\n",
    "        S_l_amenver.loc[i,'calendar_updated'] = float(S_l_amenver['calendar_updated'][i].replace(' months ago', '')) * 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are some numbers that have '$' or '\\%' that are seen as strings, so I fix this.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_vars = S_l_amenver.select_dtypes(include=['object', 'O']).copy().columns\n",
    "for col in col_vars:\n",
    "    if '$' in S_l_amenver[col].any():\n",
    "        S_l_amenver[col] = S_l_amenver[col].replace('[\\$,]', '', regex=True).astype(float)\n",
    "\n",
    "col_vars2 = S_l_amenver.select_dtypes(include=['object', 'O']).copy().columns\n",
    "for col in col_vars2:\n",
    "    if '%' in S_l_amenver[col].any():\n",
    "        S_l_amenver[col] = S_l_amenver[col].replace('[,\\%]','',regex=True).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I define some functions here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_fill(df,y_data,drop_data):\n",
    "    \"\"\"\n",
    "    df: the pandas dataframe from where the data comes from\n",
    "    y_data: the string of the dependent variable name\n",
    "    drop: A list of strings with the features you would like to drop\n",
    "    \n",
    "    Outputs:\n",
    "    X: pandas data without NaN, the independent variables\n",
    "    y: pandas data without Nan, the dependent variable\n",
    "    num_vars: list of strings with numerical variables.\n",
    "    \n",
    "    This function fills missing values with means for numerical, and mode for categorical data.\n",
    "    \"\"\"\n",
    "    drop_data.append(y_data)\n",
    "    X = df.drop(drop_data, axis=1)\n",
    "    y = df[y_data]\n",
    "    y.fillna((y.mean()), inplace=True)\n",
    "    \n",
    "    num_vars = X.select_dtypes(include=['float', 'int']).columns\n",
    "    for col in num_vars:\n",
    "        X[col].fillna((X[col].mean()), inplace=True)\n",
    "    \n",
    "    cat_vars = X.select_dtypes(include=['object', 'O']).copy().columns\n",
    "    for var in cat_vars:\n",
    "        X[var].fillna((X[var].mode()[0]), inplace=True)\n",
    "    return X, y, num_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dummies(df):\n",
    "    cat_vars = df.select_dtypes(include=['object', 'O']).copy().columns\n",
    "    for var in cat_vars:\n",
    "        # for each cat add dummy var, drop original column\n",
    "        df = pd.concat([df.drop(var, axis=1), pd.get_dummies(df[var], prefix=var, prefix_sep='_', drop_first=True)], axis=1)\n",
    "    df_2= df.copy()\n",
    "    return df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vif_dropping(X_data,VIF_lim):\n",
    "    \"\"\"\n",
    "    X_data: pandas data, independent variables without num values.\n",
    "    VIF_lim: the maximum VIF value allowed to exist\n",
    "    \n",
    "    X_out: pandas dataframe without features VIF values above VIF_lim.\n",
    "    \"\"\"\n",
    "    num_vars2 = X_data.select_dtypes(include=['float', 'int']).columns\n",
    "    variables = X_data[num_vars2]\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"VIF\"] = [variance_inflation_factor(variables.values, i) for i in range(variables.shape[1])]\n",
    "    vif_val=50\n",
    "    \n",
    "    while len(vif[vif['VIF']>VIF_lim]) != 0:\n",
    "        num_vars2 = X_data.select_dtypes(include=['float', 'int']).columns\n",
    "        vif_val=vif_val-5\n",
    "        variables = X_data[num_vars2]\n",
    "        vif = pd.DataFrame()\n",
    "        vif[\"VIF\"] = [variance_inflation_factor(variables.values, i) for i in range(variables.shape[1])]\n",
    "        vif[\"Features\"] = variables.columns\n",
    "        VIF_clean=vif[vif[\"VIF\"]>vif_val].Features\n",
    "        X_data = X_data.drop(VIF_clean,axis=1)\n",
    "        \n",
    "    X_out = X_data.copy()\n",
    "    return X_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LR_model(X_data,y_data,test_sizee, rdm_state):\n",
    "    \"\"\"\n",
    "    X_data: pandas dataframe with the independent variables\n",
    "    y_data: pandas dataframe with the dependent variable\n",
    "    test_sizee: size of the test sample\n",
    "    rdm_state: random state for randomizer.\n",
    "    \n",
    "    y_test: the test targets, as a numpy array.\n",
    "    y_test_preds: the predicted targets, as a numpy array.\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = X_data.copy()\n",
    "    scaler.fit(X_scaled)\n",
    "        \n",
    "    polynomial_features= PolynomialFeatures(degree=3)\n",
    "    x_poly = polynomial_features.fit_transform(scaler.transform(X_scaled))\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(x_poly, y_data, test_size = test_sizee, random_state=rdm_state) \n",
    "    lm_model = LinearRegression() \n",
    "    lm_model.fit(X_train, y_train) \n",
    "\n",
    "    y_test_preds = lm_model.predict(X_test) \n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, y_test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing features which do no vary.\n",
    "col_to_drop = []\n",
    "for col in S_l_amenver.columns.tolist():\n",
    "    if len(S_l_amenver[col].unique())==1:\n",
    "        col_to_drop.append(col)\n",
    "\n",
    "S_l_amenver = S_l_amenver.drop(col_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying the Linear Model with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y,num_vars = drop_fill(S_l_amenver,'price',['weekly_price','monthly_price'])\n",
    "X = make_dummies(X)\n",
    "# X = vif_dropping(X,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, y_test_preds = LR_model(X,y,0.2,42) \n",
    "\"The r-squared score for the model using all variables was {} on {} values.\".format(r2_score(y_test, y_test_preds), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVR(kernel='sigmoid',C=20,epsilon=0.1)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in normed_train_data.columns.tolist():\n",
    "    if normed_train_data[col].isna().any():\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_train_data[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "hidden_layer_size = 200\n",
    "NUM_EPOCHS = 50\n",
    "output_size = 1\n",
    "\n",
    "model = tf.keras.Sequential([    \n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu', input_shape=[len(normed_train_data.keys())]),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    \n",
    "    tf.keras.layers.Dense(output_size, activation='relu') # output layer\n",
    "])\n",
    "\n",
    "\"\"\" \n",
    "Now we have to choose the optimizer and the loss function.\n",
    "\n",
    "metrics will show us what we are interested in obtaining at each iteration.\n",
    "\"\"\"\n",
    "model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=5)\n",
    "\n",
    "model.fit(normed_train_data, \n",
    "          y_train, \n",
    "          batch_size = BATCH_SIZE, \n",
    "          epochs = NUM_EPOCHS, \n",
    "          callbacks = [early_stopping],\n",
    "          validation_split = 0.2,\n",
    "          verbose =2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3-TF2.0]",
   "language": "python",
   "name": "conda-env-py3-TF2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
