# Portfolio

##### Table of Contents  

1. [ Disaster Response Pipeline. ](#dr_pipeline)

2. [ Airbnb data project. ](#Airbnb)

3. [ Audiobook business case project. ](#audio)
   
4. [ Web application. ](#webapp)
 
5. [ Sparkify - Churn prediction. ](#sparkify)

Right now there are four projects in this portfolio and they are described below.

<a name="dr_pipeline"></a>
## Disaster Response Pipeline

The aim of this project is to showcase data engineering skills, including ETL (Extract-Transform-Load) pipeline, NLP (Natural Language Processing) and ML (Machine Learning) pipeline, using the built model in web app and applying some data visualizations. 

<a name="dr_pipeline_lib"></a>
### Libraries needed

There are several libraries needed for this project, and they are in the file `requirements.txt`. The most important libraries being: flask, pandas, sklearn, imblearn, numpy and nltk.

<a name="dr_pipeline_files"></a>
### Important files description.

- `data/disaster_messages.csv`: file with messages that can be related or not to a disaster.
- `data/disaster_categories.csv`: file classifying the messages from `data/disaster_messages.csv` within several different categories.
- `ETL Pipeline Preparation.ipynb`: The notebook used to understand the data, and do some trials on the data before finishing the ETL pipeline. Recomended to use to understand the process of building the ETL pipeline.
- `ML Pipeline Preparation.ipynb.`The notebook used to model the data, and do some ML trials on the data before finishing the ML pipeline. Recomended to use to understand the process of building the ML pipeline.
- `data/process_data.py`: ETL pipeline that cleans the data and saves it into a database.
- `models/train_classifier.py`: ML pipeline that loads the data, train a model, evaluates the model and saves the model into a pickle file.
- `models/classifier.pkl`: Saved model.
- `app/templates/*.html`: HTML templates to build the web app.
- `app/run.py`: Starts the server for the web app. 

<a name="dr_pipeline_ack"></a>
### Acknowledgements

I would like to express my gratitude to Udacity Team for the very good course.

<a name="Airbnb"></a>
## Airbnb data project:

The aim of this project is to analyse the data of Seattle's Airbnb listings obtained from the Udacity Nanodegree in Data Science. 

<a name="Airbnb_lib"></a>
### Libraries needed

The following libraries are used in this project:
1) numpy
2) pandas
3) matplotlib
4) sklearn
5) statsmodels
6) seaborn
7) re
8) os
9) imageio

<a name="Airbnb_files"></a>
### Files needed

There are several files in this project, some generated by the notebook (Airbnb_full_project.ipynb) and some used as an input for the notebook. The files used as an input for the notebook are:

1) Seattle_map2.png, to generate some images relating the map to the airbnb listings
2) Seattle_calendar.csv: data of the last ~2 millions event of bookings.
3) Seattle_listings.csv: data with the general features of all the listings in seattle up to 2017. There is a little bit less than 4000 listings.

<a name="Airbnb_ack"></a>
### Acknowledgements

I would like to express my gratitude to Simone Centellegher who provided on his github a beautiful way to plot bar graphs, which I usually find rather dull. link accessable on Jan/27/2020: https://scentellegher.github.io/visualization/2018/10/10/beautiful-bar-plots-matplotlib.html. Moreover, the file Seattle_map2.png is a printscreen of the Seattle city taken on google maps, which I have altered the quality and colors to be able to decrease file sizes.

<a name="audio"></a>
## Audiobook business case project:

This is another concise project on using Tensorflow in order to do predictions. In this case, the aim of the project is to predict the return of clients to buy audiobooks, in order to perform marketing recommendation. I used a part of a course I've done in Udemy (https://www.udemy.com/course/the-data-science-course-complete-data-science-bootcamp/) as the basis for this project. By the end of the project I show that I manage to have an accuracy of about 91% on predicting the return of the customers.

<a name="audio_lib"></a>
### Libraries needed

The following libraries are used in this project:
1) numpy
2) tensorflow
3) sklearn

<a name="audio_lib"></a>
### Files needed

There are several files in this project, some generated by the notebook Preprocessing_the_data.ipynb and some used as an input for the notebook. The files used as an input for the notebook are:

1) Audiobooks_data.csv presents the full dataset used for the whole analysis.
2) Audiobooks_data_test.npz, Audiobooks_data_train.npz and Audiobooks_data_validation are generated by the notebook.npz Preprocessing_the_data.ipynb.

In order to completely run the model, first is necessary to run the notebook Preprocessing_the_data.ipynb, followed by the notebook Predicting_client_return.ipynb.

<a name="audio_ack"></a>
### Acknowledgements

I would like to express my gratitude to 365 Careers Team for the very good course.

<a name="webapp"></a>
## Web application

This is a web application where I used the World Bank API (https://data.worldbank.org/) to generate a dashboard to do a brief analysis of the advanced education population of the top 10 world economies. Link to the web application: https://adv-educ-pop.herokuapp.com/.

<a name="webapp_lib"></a>
### Libraries needed

The following libraries are used in this project:
1) flask
2) numpy
3) pandas
4) json
5) plotly
6) collections
7) request

<a name="webapp_files"></a>
### Files needed

There are several different files needed in order to understand the web application and to add them all here would be pointless.

<a name="webapp_ack"></a>
### Acknowledgements

I would like to express my gratitude to Udacity Team for the very good course.


<a name="sparkify"></a>
## Sparkify - Churn Prediction

The project aims to predict user churns from a music app, using PySpark, showcasing my abilities with Spark. The code was written in Python 3, and the libraries used are described below. To fully understand the project, one should first read this README.md, extract the data from `mini_sparkify_event_data.zip`, then check the file `Sparkify_minidataset_EDA.ipynb` and finally check `Final_sparkify.ipynb`.

### Libraries needed

There are several libraries needed for this project, including:

- Pyspark
- Numpy
- Matplotlib
- Pandas
- Datetime
- Re
- statsmodel
- Scipy
- Seaborn


### Important files description.

- `Sparkify_minidataset_EDA.ipynb`: file with the exploratory data analysis.
- `Final_sparkify.ipynb`: file with the data cleaning, data preprocessing, feature engineering and modelling. 
- `mini_sparkify_event_data.zip`: data used in the project.


### Results and improvements.

The results show a recall of 0.82 and a F1-score of 0.84 with the model chosen. This results probably can be largely improved with i) a larger dataset (it was used data from less than 200 users); ii) time segmented data to understand the behavior of churned users just before churning, and iii) a larger gridsearch, among other possible improvements.

### Acknowledgements.

I would like to express my gratitude to Simone Centellegher who provided on his github a beautiful way to plot bar graphs, which I usually find rather dull. link accessable on Jan/27/2020: https://scentellegher.github.io/visualization/2018/10/10/beautiful-bar-plots-matplotlib.html


